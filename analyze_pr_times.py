#!/usr/bin/env python3
"""
analyze_pr_times.py

Analyze and visualize PR review time metrics from CSV data generated by gh_pr_times.py

Features:
- Overall statistics (average review times, merge times)
- Per-developer statistics
- Trend analysis showing if review times are improving or degrading
- Visual charts and graphs
- Auto-detects all CSV files in data directory

Usage:
  # Analyze all CSV files in ./data directory (default)
  python analyze_pr_times.py

  # Analyze a specific CSV file
  python analyze_pr_times.py --input results.csv

  # Analyze all CSVs in custom directory
  python analyze_pr_times.py --data-dir ./my-data --output-dir ./analytics
"""

import argparse
import csv
import glob
import os
import sys
from collections import defaultdict
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional

import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import numpy as np
from scipy import stats

# Constants
HOURS_PER_DAY = 24
DAYS_PER_MONTH = 30
MIN_PRS_DEFAULT = 3
CHART_DPI = 150
CHART_STYLE = 'seaborn-v0_8-darkgrid'
OUTLIER_CAP_REVIEW_HOURS = 200
OUTLIER_CAP_MERGE_HOURS = 500


def parse_args() -> argparse.Namespace:
    p = argparse.ArgumentParser(description="Analyze PR review time metrics from CSV")
    p.add_argument("--input", type=str, default=None, help="Input CSV file (if not provided, analyzes all CSVs in --data-dir)")
    p.add_argument("--data-dir", type=str, default="./data", help="Directory to scan for CSV files when --input not provided")
    p.add_argument("--output-dir", type=str, default="./analytics", help="Output directory for charts")
    p.add_argument("--min-prs", type=int, default=MIN_PRS_DEFAULT, help="Minimum PRs for per-developer stats")
    return p.parse_args()


def find_csv_files(data_dir: str) -> List[str]:
    """Find all CSV files in the data directory."""
    if not os.path.exists(data_dir):
        return []

    csv_files = glob.glob(os.path.join(data_dir, "*.csv"))
    return sorted(csv_files)


def load_pr_data(csv_path: str) -> List[Dict]:
    """Load PR data from CSV file."""
    prs = []
    try:
        with open(csv_path, "r", encoding="utf-8") as f:
            reader = csv.DictReader(f)
            for row in reader:
                prs.append(row)
    except FileNotFoundError:
        print(f"âŒ Error: File '{csv_path}' not found.", file=sys.stderr)
        sys.exit(1)
    return prs


def load_multiple_csv_files(csv_paths: List[str]) -> List[Dict]:
    """Load PR data from multiple CSV files and combine them."""
    all_prs = []
    for csv_path in csv_paths:
        try:
            prs = load_pr_data(csv_path)
            all_prs.extend(prs)
        except SystemExit:
            # load_pr_data calls sys.exit on FileNotFoundError, handle gracefully for multiple files
            print(f"âš ï¸  Warning: Could not read {csv_path}", file=sys.stderr)
    return all_prs


def parse_float(value: str) -> Optional[float]:
    """Safely parse float value."""
    if not value or value == "":
        return None
    try:
        return float(value)
    except (ValueError, TypeError):
        return None


def parse_datetime(value: str) -> Optional[datetime]:
    """Safely parse datetime value."""
    if not value or value == "":
        return None
    try:
        return datetime.fromisoformat(value.replace("Z", "+00:00"))
    except (ValueError, TypeError):
        return None


def hours_to_days(hours: float) -> float:
    """Convert hours to days."""
    return hours / HOURS_PER_DAY


def filter_prs_by_period(prs: List[Dict], days: Optional[int] = None) -> List[Dict]:
    """Filter PRs by time period (last N days). If days is None, return all PRs."""
    if days is None:
        return prs

    from datetime import timedelta, timezone
    cutoff_date = datetime.now(timezone.utc) - timedelta(days=days)

    filtered = []
    for pr in prs:
        created = parse_datetime(pr["created_at"])
        if created and created >= cutoff_date:
            filtered.append(pr)

    return filtered


def calculate_overall_stats(prs: List[Dict]) -> Dict:
    """Calculate overall statistics."""
    review_times = [parse_float(pr["time_to_first_review_hours"]) for pr in prs]
    review_times = [t for t in review_times if t is not None]

    merge_times = [parse_float(pr["time_to_merge_hours"]) for pr in prs]
    merge_times = [t for t in merge_times if t is not None]

    open_times = [parse_float(pr["open_time_hours"]) for pr in prs]
    open_times = [t for t in open_times if t is not None]

    total_prs = len(prs)
    merged_prs = len([pr for pr in prs if pr["merged_at"]])
    closed_not_merged_prs = len([pr for pr in prs if pr["closed_at"] and not pr["merged_at"]])
    open_prs = len([pr for pr in prs if not pr["closed_at"]])

    return {
        "total_prs": total_prs,
        "merged_prs": merged_prs,
        "closed_not_merged_prs": closed_not_merged_prs,
        "open_prs": open_prs,
        "avg_review_time": np.mean(review_times) if review_times else None,
        "median_review_time": np.median(review_times) if review_times else None,
        "avg_merge_time": np.mean(merge_times) if merge_times else None,
        "median_merge_time": np.median(merge_times) if merge_times else None,
        "avg_open_time": np.mean(open_times) if open_times else None,
    }


def calculate_per_developer_stats(prs: List[Dict], min_prs: int = 3) -> Dict[str, Dict]:
    """Calculate statistics per developer."""
    dev_data = defaultdict(lambda: {"review_times": [], "merge_times": [], "pr_count": 0})

    for pr in prs:
        author = pr.get("author", "unknown")
        if not author:
            continue

        dev_data[author]["pr_count"] += 1

        review_time = parse_float(pr["time_to_first_review_hours"])
        if review_time is not None:
            dev_data[author]["review_times"].append(review_time)

        merge_time = parse_float(pr["time_to_merge_hours"])
        if merge_time is not None:
            dev_data[author]["merge_times"].append(merge_time)

    # Calculate averages and filter by min_prs
    dev_stats = {}
    for author, data in dev_data.items():
        if data["pr_count"] >= min_prs:
            dev_stats[author] = {
                "pr_count": data["pr_count"],
                "avg_review_time": np.mean(data["review_times"]) if data["review_times"] else None,
                "avg_merge_time": np.mean(data["merge_times"]) if data["merge_times"] else None,
            }

    return dev_stats


def analyze_trends(prs: List[Dict]) -> Dict:
    """Analyze trends in review times over time."""
    # Sort PRs by creation date
    dated_prs = []
    for pr in prs:
        created = parse_datetime(pr["created_at"])
        review_time = parse_float(pr["time_to_first_review_hours"])
        merge_time = parse_float(pr["time_to_merge_hours"])
        if created:
            dated_prs.append({
                "date": created,
                "review_time": review_time,
                "merge_time": merge_time,
            })

    dated_prs.sort(key=lambda x: x["date"])

    if len(dated_prs) < 2:
        return {"trend": "insufficient_data"}

    # Calculate trend using linear regression
    dates_numeric = [(pr["date"] - dated_prs[0]["date"]).days for pr in dated_prs]

    # Review time trend
    review_data = [(dates_numeric[i], pr["review_time"])
                   for i, pr in enumerate(dated_prs) if pr["review_time"] is not None]

    review_slope = None
    if len(review_data) >= 2:
        x, y = zip(*review_data)
        slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)
        review_slope = slope

    # Merge time trend
    merge_data = [(dates_numeric[i], pr["merge_time"])
                  for i, pr in enumerate(dated_prs) if pr["merge_time"] is not None]

    merge_slope = None
    if len(merge_data) >= 2:
        x, y = zip(*merge_data)
        slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)
        merge_slope = slope

    return {
        "review_slope": review_slope,
        "merge_slope": merge_slope,
        "dated_prs": dated_prs,
    }


def print_time_period_stats(prs: List[Dict], period_name: str, period_days: Optional[int] = None) -> None:
    """Print statistics for a specific time period."""
    filtered_prs = filter_prs_by_period(prs, period_days)

    if not filtered_prs:
        print(f"\nğŸ“Š {period_name.upper()} - No PRs in this period")
        return

    stats = calculate_overall_stats(filtered_prs)

    print(f"\nğŸ“Š {period_name.upper()}")
    print("-" * 80)
    print(f"PRs in period:          {stats['total_prs']}")
    print(f"  â””â”€ Merged:            {stats['merged_prs']}")
    print(f"  â””â”€ Closed (not merged): {stats['closed_not_merged_prs']}")
    print(f"  â””â”€ Still Open:        {stats['open_prs']}")

    if stats["avg_review_time"]:
        print(f"\nTime to First Review:   {stats['avg_review_time']:.2f} hrs ({hours_to_days(stats['avg_review_time']):.1f} days) avg, {stats['median_review_time']:.2f} hrs median")

    if stats["avg_merge_time"]:
        print(f"Time to Merge:          {stats['avg_merge_time']:.2f} hrs ({hours_to_days(stats['avg_merge_time']):.1f} days) avg, {stats['median_merge_time']:.2f} hrs median")
        if stats["avg_review_time"]:
            review_to_merge = stats['avg_merge_time'] - stats['avg_review_time']
            print(f"Review â†’ Merge:         {review_to_merge:.2f} hrs ({hours_to_days(review_to_merge):.1f} days)")


def print_statistics(overall: Dict, per_dev: Dict, trends: Dict) -> None:
    """Print statistics to console."""
    print("\n" + "=" * 80)
    print("ğŸ“Š PR REVIEW TIME ANALYSIS")
    print("=" * 80)

    # Overall stats
    print("\nğŸ“ˆ OVERALL STATISTICS")
    print("-" * 80)
    print(f"Total PRs:              {overall['total_prs']}")
    print(f"  â””â”€ Merged:            {overall['merged_prs']}")
    print(f"  â””â”€ Closed (not merged): {overall['closed_not_merged_prs']}")
    print(f"  â””â”€ Still Open:        {overall['open_prs']}")

    if overall["avg_review_time"]:
        print(f"\nTime to First Review (creation â†’ first review):")
        print(f"  â””â”€ Average:           {overall['avg_review_time']:.2f} hours ({hours_to_days(overall['avg_review_time']):.1f} days)")
        print(f"  â””â”€ Median:            {overall['median_review_time']:.2f} hours ({hours_to_days(overall['median_review_time']):.1f} days)")

    if overall["avg_merge_time"]:
        print(f"\nTime to Merge (creation â†’ merge):")
        print(f"  â””â”€ Average:           {overall['avg_merge_time']:.2f} hours ({hours_to_days(overall['avg_merge_time']):.1f} days)")
        print(f"  â””â”€ Median:            {overall['median_merge_time']:.2f} hours ({hours_to_days(overall['median_merge_time']):.1f} days)")

        if overall["avg_review_time"]:
            review_to_merge = overall['avg_merge_time'] - overall['avg_review_time']
            print(f"  â””â”€ Avg time from first review to merge: {review_to_merge:.2f} hours ({hours_to_days(review_to_merge):.1f} days)")

    # Trends
    print("\nğŸ“‰ TRENDS")
    print("-" * 80)
    if trends.get("review_slope") is not None:
        review_direction = "ğŸ”´ INCREASING" if trends["review_slope"] > 0 else "ğŸŸ¢ DECREASING"
        days_change = abs(trends["review_slope"]) * DAYS_PER_MONTH
        print(f"Review Time Trend:      {review_direction}")
        print(f"  â””â”€ Change per month:  {days_change:.2f} hours")

    if trends.get("merge_slope") is not None:
        merge_direction = "ğŸ”´ INCREASING" if trends["merge_slope"] > 0 else "ğŸŸ¢ DECREASING"
        days_change = abs(trends["merge_slope"]) * DAYS_PER_MONTH
        print(f"Merge Time Trend:       {merge_direction}")
        print(f"  â””â”€ Change per month:  {days_change:.2f} hours")

    # Per developer
    if per_dev:
        print("\nğŸ‘¥ PER DEVELOPER STATISTICS")
        print("-" * 80)
        # Sort by PR count
        sorted_devs = sorted(per_dev.items(), key=lambda x: x[1]["pr_count"], reverse=True)

        for author, stats in sorted_devs:
            print(f"\n{author}:")
            print(f"  â””â”€ PRs:               {stats['pr_count']}")
            if stats["avg_review_time"] is not None:
                print(f"  â””â”€ Avg Time to 1st Review: {stats['avg_review_time']:.2f} hours ({hours_to_days(stats['avg_review_time']):.1f} days)")
            if stats["avg_merge_time"] is not None:
                print(f"  â””â”€ Avg Time to Merge: {stats['avg_merge_time']:.2f} hours ({hours_to_days(stats['avg_merge_time']):.1f} days)")
                if stats["avg_review_time"] is not None:
                    review_to_merge = stats['avg_merge_time'] - stats['avg_review_time']
                    print(f"  â””â”€ Avg Review â†’ Merge: {review_to_merge:.2f} hours ({hours_to_days(review_to_merge):.1f} days)")

    print("\n" + "=" * 80)


def _format_time_axis(ax):
    """Format axis for time-based charts."""
    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))
    ax.xaxis.set_major_locator(mdates.AutoDateLocator())
    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')


def _add_trend_line(ax, dates, values, slope):
    """Add trend line to a scatter plot."""
    dates_numeric = [(d - dates[0]).days for d in dates]
    intercept = np.mean(values) - slope * np.mean(dates_numeric)
    trend_line = [slope * d + intercept for d in dates_numeric]
    ax.plot(dates, trend_line, 'r--', linewidth=2, label=f'Trend: {slope*DAYS_PER_MONTH:.2f} hrs/month')


def _create_histogram(ax, values, color, title, outlier_cap):
    """Create a histogram with mean and median lines."""
    filtered_values = [v for v in values if v is not None and v < outlier_cap]

    if filtered_values:
        ax.hist(filtered_values, bins=30, color=color, edgecolor='black', alpha=0.7)
        ax.axvline(np.mean(filtered_values), color='red', linestyle='--', linewidth=2, label=f'Mean: {np.mean(filtered_values):.1f}h')
        ax.axvline(np.median(filtered_values), color='green', linestyle='--', linewidth=2, label=f'Median: {np.median(filtered_values):.1f}h')
        ax.set_xlabel('Hours', fontsize=12)
        ax.set_ylabel('Frequency', fontsize=12)
        ax.set_title(title, fontsize=14, fontweight='bold')
        ax.legend()


def create_visualizations(prs: List[Dict], overall: Dict, per_dev: Dict, trends: Dict, output_dir: str) -> None:
    """Create visualization charts."""
    Path(output_dir).mkdir(parents=True, exist_ok=True)

    # Set style
    plt.style.use(CHART_STYLE)

    # 1. Review Time Trend Over Time
    dated_prs = trends.get("dated_prs", [])
    if dated_prs:
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))

        # Review times
        dates = [pr["date"] for pr in dated_prs if pr["review_time"] is not None]
        review_times = [pr["review_time"] for pr in dated_prs if pr["review_time"] is not None]

        if dates and review_times:
            ax1.scatter(dates, review_times, alpha=0.6, s=30)

            # Add trend line
            if trends.get("review_slope") is not None:
                _add_trend_line(ax1, dates, review_times, trends["review_slope"])

            ax1.set_ylabel('Hours', fontsize=12)
            ax1.set_title('Time to First Review Over Time', fontsize=14, fontweight='bold')
            ax1.legend()
            _format_time_axis(ax1)

        # Merge times
        merge_dates = [pr["date"] for pr in dated_prs if pr["merge_time"] is not None]
        merge_times = [pr["merge_time"] for pr in dated_prs if pr["merge_time"] is not None]

        if merge_dates and merge_times:
            ax2.scatter(merge_dates, merge_times, alpha=0.6, s=30, color='green')

            # Add trend line
            if trends.get("merge_slope") is not None:
                _add_trend_line(ax2, merge_dates, merge_times, trends["merge_slope"])

            ax2.set_xlabel('Date', fontsize=12)
            ax2.set_ylabel('Hours', fontsize=12)
            ax2.set_title('Time to Merge Over Time', fontsize=14, fontweight='bold')
            ax2.legend()
            _format_time_axis(ax2)

        plt.tight_layout()
        plt.savefig(f"{output_dir}/trends_over_time.png", dpi=CHART_DPI, bbox_inches='tight')
        print(f"âœ“ Saved: {output_dir}/trends_over_time.png")
        plt.close()

    # 2. Per Developer Comparison
    if per_dev:
        sorted_devs = sorted(per_dev.items(), key=lambda x: x[1]["pr_count"], reverse=True)
        authors = [author for author, _ in sorted_devs]
        review_times = [stats["avg_review_time"] or 0 for _, stats in sorted_devs]
        merge_times = [stats["avg_merge_time"] or 0 for _, stats in sorted_devs]

        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))

        # Review times
        ax1.barh(authors, review_times, color='skyblue')
        ax1.set_xlabel('Average Hours to First Review', fontsize=12)
        ax1.set_title('Average Review Time by Developer', fontsize=14, fontweight='bold')
        ax1.invert_yaxis()

        # Merge times
        ax2.barh(authors, merge_times, color='lightgreen')
        ax2.set_xlabel('Average Hours to Merge', fontsize=12)
        ax2.set_title('Average Merge Time by Developer', fontsize=14, fontweight='bold')
        ax2.invert_yaxis()

        plt.tight_layout()
        plt.savefig(f"{output_dir}/per_developer_stats.png", dpi=CHART_DPI, bbox_inches='tight')
        print(f"âœ“ Saved: {output_dir}/per_developer_stats.png")
        plt.close()

    # 3. Distribution Histograms
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

    review_times = [parse_float(pr["time_to_first_review_hours"]) for pr in prs]
    _create_histogram(ax1, review_times, 'skyblue', 'Distribution of Review Times', OUTLIER_CAP_REVIEW_HOURS)

    merge_times = [parse_float(pr["time_to_merge_hours"]) for pr in prs]
    _create_histogram(ax2, merge_times, 'lightgreen', 'Distribution of Merge Times', OUTLIER_CAP_MERGE_HOURS)

    plt.tight_layout()
    plt.savefig(f"{output_dir}/distributions.png", dpi=CHART_DPI, bbox_inches='tight')
    print(f"âœ“ Saved: {output_dir}/distributions.png")
    plt.close()


def main() -> None:
    args = parse_args()

    # Determine which CSV files to analyze
    csv_files = []
    if args.input:
        # Single file specified
        csv_files = [args.input]
        print(f"ğŸ” Loading PR data from {args.input}...")
    else:
        # Auto-detect CSV files in data directory
        print(f"ğŸ” Scanning {args.data_dir} for CSV files...")
        csv_files = find_csv_files(args.data_dir)

        if not csv_files:
            print(f"âŒ No CSV files found in {args.data_dir}", file=sys.stderr)
            print(f"   Run 'python gh_pr_times.py --repos owner/repo' first to fetch data", file=sys.stderr)
            sys.exit(1)

        print(f"âœ“ Found {len(csv_files)} CSV file(s):")
        for csv_file in csv_files:
            print(f"  - {os.path.basename(csv_file)}")

    # Load data from CSV file(s)
    if len(csv_files) == 1:
        prs = load_pr_data(csv_files[0])
    else:
        print(f"\nğŸ“Š Combining data from {len(csv_files)} repositories...")
        prs = load_multiple_csv_files(csv_files)

    if not prs:
        print("âŒ No PR data found in CSV file(s).", file=sys.stderr)
        sys.exit(1)

    print(f"âœ“ Loaded {len(prs)} PRs total")

    print("\nğŸ“Š Calculating statistics...")
    overall = calculate_overall_stats(prs)
    per_dev = calculate_per_developer_stats(prs, args.min_prs)
    trends = analyze_trends(prs)

    print_statistics(overall, per_dev, trends)

    # Print time period analysis
    print("\n" + "=" * 80)
    print("ğŸ“… TIME PERIOD ANALYSIS")
    print("=" * 80)
    print_time_period_stats(prs, "Last 30 Days", 30)
    print_time_period_stats(prs, "Last Quarter (90 Days)", 90)

    print("\nğŸ“ˆ Creating visualizations...")
    create_visualizations(prs, overall, per_dev, trends, args.output_dir)

    print(f"\nâœ… Analysis complete! Charts saved to '{args.output_dir}/'")


if __name__ == "__main__":
    main()
