#!/usr/bin/env python3
"""
analyze_pr_times.py

Analyze and visualize PR review time metrics from CSV data generated by gh_pr_times.py

Features:
- Overall statistics (average review times, merge times)
- Per-developer statistics
- Trend analysis showing if review times are improving or degrading
- Visual charts and graphs
- Auto-detects all CSV files in data directory

Usage:
  # Analyze all CSV files in ./data directory (default)
  python analyze_pr_times.py

  # Analyze a specific CSV file
  python analyze_pr_times.py --input results.csv

  # Analyze all CSVs in custom directory
  python analyze_pr_times.py --data-dir ./my-data --output-dir ./analytics
"""

import argparse
import csv
import glob
import json
import os
import sys
from collections import defaultdict
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional

import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import numpy as np
from scipy import stats

# Constants
HOURS_PER_DAY = 24
DAYS_PER_MONTH = 30
MIN_PRS_DEFAULT = 3
CHART_DPI = 150
CHART_STYLE = 'seaborn-v0_8-darkgrid'
OUTLIER_CAP_REVIEW_HOURS = 200
OUTLIER_CAP_MERGE_HOURS = 500

# PR Size thresholds (configurable)
PR_SIZE_SMALL = 200  # lines changed
PR_SIZE_MEDIUM = 500  # lines changed

# Time periods for analysis (ordered by time descending)
TIME_PERIODS = {
    "last_7_days": {"name": "Last 7 Days", "days": 7},
    "last_30_days": {"name": "Last 30 Days", "days": 30},
    "last_quarter": {"name": "Last Quarter", "days": 90},
    "overall": {"name": "Overall", "days": None}
}


def parse_args() -> argparse.Namespace:
    p = argparse.ArgumentParser(description="Analyze PR review time metrics from CSV")
    p.add_argument("--input", type=str, default=None, help="Input CSV file (if not provided, analyzes all CSVs in --data-dir)")
    p.add_argument("--data-dir", type=str, default="./data", help="Directory to scan for CSV files when --input not provided")
    p.add_argument("--output-dir", type=str, default="./report", help="Output directory for charts and JSON data")
    p.add_argument("--min-prs", type=int, default=MIN_PRS_DEFAULT, help="Minimum PRs for per-developer stats")
    return p.parse_args()


def find_csv_files(data_dir: str) -> List[str]:
    """Find all CSV files in the data directory."""
    if not os.path.exists(data_dir):
        return []

    csv_files = glob.glob(os.path.join(data_dir, "*.csv"))
    return sorted(csv_files)


def load_pr_data(csv_path: str) -> List[Dict]:
    """Load PR data from CSV file."""
    prs = []
    try:
        with open(csv_path, "r", encoding="utf-8") as f:
            reader = csv.DictReader(f)
            for row in reader:
                prs.append(row)
    except FileNotFoundError:
        print(f"❌ Error: File '{csv_path}' not found.", file=sys.stderr)
        sys.exit(1)
    return prs


def load_prs_by_repository(csv_paths: List[str]) -> Dict[str, List[Dict]]:
    """Load PR data organized by repository."""
    prs_by_repo = {}

    for csv_path in csv_paths:
        try:
            prs = load_pr_data(csv_path)
            if prs:
                # Get repository name from first PR's repo field, or from filename
                repo_name = prs[0].get("repo", os.path.basename(csv_path).replace(".csv", "").replace("_", "/"))
                prs_by_repo[repo_name] = prs
        except SystemExit:
            # load_pr_data calls sys.exit on FileNotFoundError, handle gracefully for multiple files
            print(f"⚠️  Warning: Could not read {csv_path}", file=sys.stderr)

    return prs_by_repo


def load_multiple_csv_files(csv_paths: List[str]) -> List[Dict]:
    """Load PR data from multiple CSV files and combine them."""
    all_prs = []
    for csv_path in csv_paths:
        try:
            prs = load_pr_data(csv_path)
            all_prs.extend(prs)
        except SystemExit:
            # load_pr_data calls sys.exit on FileNotFoundError, handle gracefully for multiple files
            print(f"⚠️  Warning: Could not read {csv_path}", file=sys.stderr)
    return all_prs


def parse_float(value: str) -> Optional[float]:
    """Safely parse float value."""
    if not value or value == "":
        return None
    try:
        return float(value)
    except (ValueError, TypeError):
        return None


def parse_datetime(value: str) -> Optional[datetime]:
    """Safely parse datetime value."""
    if not value or value == "":
        return None
    try:
        return datetime.fromisoformat(value.replace("Z", "+00:00"))
    except (ValueError, TypeError):
        return None


def hours_to_days(hours: float) -> float:
    """Convert hours to days."""
    return hours / HOURS_PER_DAY


def get_pr_size(pr: Dict) -> str:
    """Categorize PR size based on lines changed."""
    additions = parse_float(pr.get("additions", "0")) or 0
    deletions = parse_float(pr.get("deletions", "0")) or 0
    total_changes = additions + deletions

    if total_changes < PR_SIZE_SMALL:
        return "small"
    elif total_changes < PR_SIZE_MEDIUM:
        return "medium"
    else:
        return "large"


def parse_author_list(author_str: str) -> List[str]:
    """Parse comma-separated author list from CSV."""
    if not author_str or author_str == "":
        return []
    return [a.strip() for a in author_str.split(",") if a.strip()]


def parse_comment_counts(comment_str: str) -> Dict[str, int]:
    """Parse comment counts string like 'loktar00:3,peter:4' into dict."""
    if not comment_str or comment_str == "":
        return {}

    result = {}
    for item in comment_str.split(","):
        item = item.strip()
        if ":" in item:
            author, count = item.split(":", 1)
            try:
                result[author.strip()] = int(count.strip())
            except ValueError:
                pass  # Skip malformed entries
    return result


def filter_prs_by_period(prs: List[Dict], days: Optional[int] = None) -> List[Dict]:
    """Filter PRs by time period (last N days). If days is None, return all PRs."""
    if days is None:
        return prs

    from datetime import timedelta, timezone
    cutoff_date = datetime.now(timezone.utc) - timedelta(days=days)

    filtered = []
    for pr in prs:
        created = parse_datetime(pr["created_at"])
        if created and created >= cutoff_date:
            filtered.append(pr)

    return filtered


def calculate_overall_stats(prs: List[Dict]) -> Dict:
    """Calculate overall statistics."""
    review_times = [parse_float(pr["time_to_first_review_hours"]) for pr in prs]
    review_times = [t for t in review_times if t is not None]

    merge_times = [parse_float(pr["time_to_merge_hours"]) for pr in prs]
    merge_times = [t for t in merge_times if t is not None]

    open_times = [parse_float(pr["open_time_hours"]) for pr in prs]
    open_times = [t for t in open_times if t is not None]

    total_prs = len(prs)
    merged_prs = len([pr for pr in prs if pr["merged_at"]])
    closed_not_merged_prs = len([pr for pr in prs if pr["closed_at"] and not pr["merged_at"]])
    open_prs = len([pr for pr in prs if not pr["closed_at"]])

    return {
        "total_prs": total_prs,
        "merged_prs": merged_prs,
        "closed_not_merged_prs": closed_not_merged_prs,
        "open_prs": open_prs,
        "avg_review_time": np.mean(review_times) if review_times else None,
        "median_review_time": np.median(review_times) if review_times else None,
        "avg_merge_time": np.mean(merge_times) if merge_times else None,
        "median_merge_time": np.median(merge_times) if merge_times else None,
        "avg_open_time": np.mean(open_times) if open_times else None,
    }


def calculate_per_developer_stats(prs: List[Dict], min_prs: int = 3) -> Dict[str, Dict]:
    """Calculate statistics per developer."""
    dev_data = defaultdict(lambda: {"review_times": [], "merge_times": [], "pr_count": 0})

    for pr in prs:
        author = pr.get("author", "unknown")
        if not author:
            continue

        dev_data[author]["pr_count"] += 1

        review_time = parse_float(pr["time_to_first_review_hours"])
        if review_time is not None:
            dev_data[author]["review_times"].append(review_time)

        merge_time = parse_float(pr["time_to_merge_hours"])
        if merge_time is not None:
            dev_data[author]["merge_times"].append(merge_time)

    # Calculate averages and filter by min_prs
    dev_stats = {}
    for author, data in dev_data.items():
        if data["pr_count"] >= min_prs:
            dev_stats[author] = {
                "pr_count": data["pr_count"],
                "avg_review_time": np.mean(data["review_times"]) if data["review_times"] else None,
                "avg_merge_time": np.mean(data["merge_times"]) if data["merge_times"] else None,
            }

    return dev_stats


def analyze_trends(prs: List[Dict]) -> Dict:
    """Analyze trends in review times over time."""
    # Sort PRs by creation date
    dated_prs = []
    for pr in prs:
        created = parse_datetime(pr["created_at"])
        review_time = parse_float(pr["time_to_first_review_hours"])
        merge_time = parse_float(pr["time_to_merge_hours"])
        if created:
            dated_prs.append({
                "date": created,
                "review_time": review_time,
                "merge_time": merge_time,
            })

    dated_prs.sort(key=lambda x: x["date"])

    if len(dated_prs) < 2:
        return {"trend": "insufficient_data"}

    # Calculate trend using linear regression
    dates_numeric = [(pr["date"] - dated_prs[0]["date"]).days for pr in dated_prs]

    # Review time trend
    review_data = [(dates_numeric[i], pr["review_time"])
                   for i, pr in enumerate(dated_prs) if pr["review_time"] is not None]

    review_slope = None
    if len(review_data) >= 2:
        x, y = zip(*review_data)
        slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)
        review_slope = slope

    # Merge time trend
    merge_data = [(dates_numeric[i], pr["merge_time"])
                  for i, pr in enumerate(dated_prs) if pr["merge_time"] is not None]

    merge_slope = None
    if len(merge_data) >= 2:
        x, y = zip(*merge_data)
        slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)
        merge_slope = slope

    return {
        "review_slope": review_slope,
        "merge_slope": merge_slope,
        "dated_prs": dated_prs,
    }


def print_time_period_stats(prs: List[Dict], period_name: str, period_days: Optional[int] = None) -> None:
    """Print statistics for a specific time period."""
    filtered_prs = filter_prs_by_period(prs, period_days)

    if not filtered_prs:
        print(f"\n📊 {period_name.upper()} - No PRs in this period")
        return

    stats = calculate_overall_stats(filtered_prs)

    print(f"\n📊 {period_name.upper()}")
    print("-" * 80)
    print(f"PRs in period:          {stats['total_prs']}")
    print(f"  └─ Merged:            {stats['merged_prs']}")
    print(f"  └─ Closed (not merged): {stats['closed_not_merged_prs']}")
    print(f"  └─ Still Open:        {stats['open_prs']}")

    if stats["avg_review_time"]:
        print(f"\nTime to First Review:   {stats['avg_review_time']:.2f} hrs ({hours_to_days(stats['avg_review_time']):.1f} days) avg, {stats['median_review_time']:.2f} hrs median")

    if stats["avg_merge_time"]:
        print(f"Time to Merge:          {stats['avg_merge_time']:.2f} hrs ({hours_to_days(stats['avg_merge_time']):.1f} days) avg, {stats['median_merge_time']:.2f} hrs median")
        if stats["avg_review_time"]:
            review_to_merge = stats['avg_merge_time'] - stats['avg_review_time']
            print(f"Review → Merge:         {review_to_merge:.2f} hrs ({hours_to_days(review_to_merge):.1f} days)")


def print_statistics(overall: Dict, per_dev: Dict, trends: Dict) -> None:
    """Print statistics to console."""
    print("\n" + "=" * 80)
    print("📊 PR REVIEW TIME ANALYSIS")
    print("=" * 80)

    # Overall stats
    print("\n📈 OVERALL STATISTICS")
    print("-" * 80)
    print(f"Total PRs:              {overall['total_prs']}")
    print(f"  └─ Merged:            {overall['merged_prs']}")
    print(f"  └─ Closed (not merged): {overall['closed_not_merged_prs']}")
    print(f"  └─ Still Open:        {overall['open_prs']}")

    if overall["avg_review_time"]:
        print(f"\nTime to First Review (creation → first review):")
        print(f"  └─ Average:           {overall['avg_review_time']:.2f} hours ({hours_to_days(overall['avg_review_time']):.1f} days)")
        print(f"  └─ Median:            {overall['median_review_time']:.2f} hours ({hours_to_days(overall['median_review_time']):.1f} days)")

    if overall["avg_merge_time"]:
        print(f"\nTime to Merge (creation → merge):")
        print(f"  └─ Average:           {overall['avg_merge_time']:.2f} hours ({hours_to_days(overall['avg_merge_time']):.1f} days)")
        print(f"  └─ Median:            {overall['median_merge_time']:.2f} hours ({hours_to_days(overall['median_merge_time']):.1f} days)")

        if overall["avg_review_time"]:
            review_to_merge = overall['avg_merge_time'] - overall['avg_review_time']
            print(f"  └─ Avg time from first review to merge: {review_to_merge:.2f} hours ({hours_to_days(review_to_merge):.1f} days)")

    # Trends
    print("\n📉 TRENDS")
    print("-" * 80)
    if trends.get("review_slope") is not None:
        review_direction = "🔴 INCREASING" if trends["review_slope"] > 0 else "🟢 DECREASING"
        days_change = abs(trends["review_slope"]) * DAYS_PER_MONTH
        print(f"Review Time Trend:      {review_direction}")
        print(f"  └─ Change per month:  {days_change:.2f} hours")

    if trends.get("merge_slope") is not None:
        merge_direction = "🔴 INCREASING" if trends["merge_slope"] > 0 else "🟢 DECREASING"
        days_change = abs(trends["merge_slope"]) * DAYS_PER_MONTH
        print(f"Merge Time Trend:       {merge_direction}")
        print(f"  └─ Change per month:  {days_change:.2f} hours")

    # Per developer
    if per_dev:
        print("\n👥 PER DEVELOPER STATISTICS")
        print("-" * 80)
        # Sort by PR count
        sorted_devs = sorted(per_dev.items(), key=lambda x: x[1]["pr_count"], reverse=True)

        for author, stats in sorted_devs:
            print(f"\n{author}:")
            print(f"  └─ PRs:               {stats['pr_count']}")
            if stats["avg_review_time"] is not None:
                print(f"  └─ Avg Time to 1st Review: {stats['avg_review_time']:.2f} hours ({hours_to_days(stats['avg_review_time']):.1f} days)")
            if stats["avg_merge_time"] is not None:
                print(f"  └─ Avg Time to Merge: {stats['avg_merge_time']:.2f} hours ({hours_to_days(stats['avg_merge_time']):.1f} days)")
                if stats["avg_review_time"] is not None:
                    review_to_merge = stats['avg_merge_time'] - stats['avg_review_time']
                    print(f"  └─ Avg Review → Merge: {review_to_merge:.2f} hours ({hours_to_days(review_to_merge):.1f} days)")

    print("\n" + "=" * 80)


def _format_time_axis(ax):
    """Format axis for time-based charts."""
    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))
    ax.xaxis.set_major_locator(mdates.AutoDateLocator())
    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')


def _add_trend_line(ax, dates, values, slope):
    """Add trend line to a scatter plot."""
    dates_numeric = [(d - dates[0]).days for d in dates]
    intercept = np.mean(values) - slope * np.mean(dates_numeric)
    trend_line = [slope * d + intercept for d in dates_numeric]
    ax.plot(dates, trend_line, 'r--', linewidth=2, label=f'Trend: {slope*DAYS_PER_MONTH:.2f} hrs/month')


def _create_histogram(ax, values, color, title, outlier_cap):
    """Create a histogram with mean and median lines."""
    filtered_values = [v for v in values if v is not None and v < outlier_cap]

    if filtered_values:
        ax.hist(filtered_values, bins=30, color=color, edgecolor='black', alpha=0.7)
        ax.axvline(np.mean(filtered_values), color='red', linestyle='--', linewidth=2, label=f'Mean: {np.mean(filtered_values):.1f}h')
        ax.axvline(np.median(filtered_values), color='green', linestyle='--', linewidth=2, label=f'Median: {np.median(filtered_values):.1f}h')
        ax.set_xlabel('Hours', fontsize=12)
        ax.set_ylabel('Frequency', fontsize=12)
        ax.set_title(title, fontsize=14, fontweight='bold')
        ax.legend()


def create_period_visualizations(prs: List[Dict], period_key: str, period_name: str, output_dir: str, repo_key: str = "") -> Dict[str, str]:
    """Create visualization charts for a specific time period and repository, return chart filenames."""
    # Ensure output directory exists
    Path(output_dir).mkdir(parents=True, exist_ok=True)

    chart_files = {}

    # Create safe filename prefix for repo
    repo_prefix = repo_key.replace("/", "_").replace(" ", "_") + "_" if repo_key else ""

    # Title suffix for charts
    title_suffix = f" - {period_name}" if period_name else ""

    # 1. Trends Over Time
    overall = calculate_overall_stats(prs)
    trends = analyze_trends(prs)
    dated_prs = trends.get("dated_prs", [])

    if dated_prs:
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))

        # Review times
        dates = [pr["date"] for pr in dated_prs if pr["review_time"] is not None]
        review_times = [pr["review_time"] for pr in dated_prs if pr["review_time"] is not None]

        if dates and review_times:
            ax1.scatter(dates, review_times, alpha=0.6, s=30)
            if trends.get("review_slope") is not None:
                _add_trend_line(ax1, dates, review_times, trends["review_slope"])
            ax1.set_ylabel('Hours', fontsize=12)
            ax1.set_title(f'Time to First Review{title_suffix}', fontsize=14, fontweight='bold')
            ax1.legend()
            _format_time_axis(ax1)

        # Merge times
        merge_dates = [pr["date"] for pr in dated_prs if pr["merge_time"] is not None]
        merge_times = [pr["merge_time"] for pr in dated_prs if pr["merge_time"] is not None]

        if merge_dates and merge_times:
            ax2.scatter(merge_dates, merge_times, alpha=0.6, s=30, color='green')
            if trends.get("merge_slope") is not None:
                _add_trend_line(ax2, merge_dates, merge_times, trends["merge_slope"])
            ax2.set_xlabel('Date', fontsize=12)
            ax2.set_ylabel('Hours', fontsize=12)
            ax2.set_title(f'Time to Merge{title_suffix}', fontsize=14, fontweight='bold')
            ax2.legend()
            _format_time_axis(ax2)

        plt.tight_layout()
        filename = f"{repo_prefix}trends_{period_key}.png"
        filepath = f"{output_dir}/{filename}"
        plt.savefig(filepath, dpi=CHART_DPI, bbox_inches='tight')
        chart_files['trends'] = filename
        plt.close()

    # 2. Distribution Histograms
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

    review_times = [parse_float(pr["time_to_first_review_hours"]) for pr in prs]
    _create_histogram(ax1, review_times, 'skyblue', f'Review Times Distribution{title_suffix if dated_prs else ""}', OUTLIER_CAP_REVIEW_HOURS)

    merge_times = [parse_float(pr["time_to_merge_hours"]) for pr in prs]
    _create_histogram(ax2, merge_times, 'lightgreen', f'Merge Times Distribution{title_suffix if dated_prs else ""}', OUTLIER_CAP_MERGE_HOURS)

    plt.tight_layout()
    filename = f"{repo_prefix}distributions_{period_key}.png"
    filepath = f"{output_dir}/{filename}"
    plt.savefig(filepath, dpi=CHART_DPI, bbox_inches='tight')
    chart_files['distributions'] = filename
    plt.close()

    return chart_files


def create_visualizations(prs: List[Dict], overall: Dict, per_dev: Dict, trends: Dict, output_dir: str) -> None:
    """Create visualization charts."""
    Path(output_dir).mkdir(parents=True, exist_ok=True)

    # Set style
    plt.style.use(CHART_STYLE)

    # 1. Review Time Trend Over Time
    dated_prs = trends.get("dated_prs", [])
    if dated_prs:
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))

        # Review times
        dates = [pr["date"] for pr in dated_prs if pr["review_time"] is not None]
        review_times = [pr["review_time"] for pr in dated_prs if pr["review_time"] is not None]

        if dates and review_times:
            ax1.scatter(dates, review_times, alpha=0.6, s=30)

            # Add trend line
            if trends.get("review_slope") is not None:
                _add_trend_line(ax1, dates, review_times, trends["review_slope"])

            ax1.set_ylabel('Hours', fontsize=12)
            ax1.set_title('Time to First Review Over Time', fontsize=14, fontweight='bold')
            ax1.legend()
            _format_time_axis(ax1)

        # Merge times
        merge_dates = [pr["date"] for pr in dated_prs if pr["merge_time"] is not None]
        merge_times = [pr["merge_time"] for pr in dated_prs if pr["merge_time"] is not None]

        if merge_dates and merge_times:
            ax2.scatter(merge_dates, merge_times, alpha=0.6, s=30, color='green')

            # Add trend line
            if trends.get("merge_slope") is not None:
                _add_trend_line(ax2, merge_dates, merge_times, trends["merge_slope"])

            ax2.set_xlabel('Date', fontsize=12)
            ax2.set_ylabel('Hours', fontsize=12)
            ax2.set_title('Time to Merge Over Time', fontsize=14, fontweight='bold')
            ax2.legend()
            _format_time_axis(ax2)

        plt.tight_layout()
        plt.savefig(f"{output_dir}/trends_over_time.png", dpi=CHART_DPI, bbox_inches='tight')
        print(f"✓ Saved: {output_dir}/trends_over_time.png")
        plt.close()

    # 2. Per Developer Comparison
    if per_dev:
        sorted_devs = sorted(per_dev.items(), key=lambda x: x[1]["pr_count"], reverse=True)
        authors = [author for author, _ in sorted_devs]
        review_times = [stats["avg_review_time"] or 0 for _, stats in sorted_devs]
        merge_times = [stats["avg_merge_time"] or 0 for _, stats in sorted_devs]

        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))

        # Review times
        ax1.barh(authors, review_times, color='skyblue')
        ax1.set_xlabel('Average Hours to First Review', fontsize=12)
        ax1.set_title('Average Review Time by Developer', fontsize=14, fontweight='bold')
        ax1.invert_yaxis()

        # Merge times
        ax2.barh(authors, merge_times, color='lightgreen')
        ax2.set_xlabel('Average Hours to Merge', fontsize=12)
        ax2.set_title('Average Merge Time by Developer', fontsize=14, fontweight='bold')
        ax2.invert_yaxis()

        plt.tight_layout()
        plt.savefig(f"{output_dir}/per_developer_stats.png", dpi=CHART_DPI, bbox_inches='tight')
        print(f"✓ Saved: {output_dir}/per_developer_stats.png")
        plt.close()

    # 3. Distribution Histograms
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

    review_times = [parse_float(pr["time_to_first_review_hours"]) for pr in prs]
    _create_histogram(ax1, review_times, 'skyblue', 'Distribution of Review Times', OUTLIER_CAP_REVIEW_HOURS)

    merge_times = [parse_float(pr["time_to_merge_hours"]) for pr in prs]
    _create_histogram(ax2, merge_times, 'lightgreen', 'Distribution of Merge Times', OUTLIER_CAP_MERGE_HOURS)

    plt.tight_layout()
    plt.savefig(f"{output_dir}/distributions.png", dpi=CHART_DPI, bbox_inches='tight')
    print(f"✓ Saved: {output_dir}/distributions.png")
    plt.close()


def generate_repo_data(repo_name: str, prs: List[Dict], output_dir: str, is_combined: bool = False, repo_id: str = None) -> Dict:
    """Generate data structure for a single repository or combined view."""
    # Calculate stats for all time periods
    period_stats = {}
    period_charts = {}

    # Create safe repo key for filenames
    repo_key = repo_name.replace("/", "_").replace(" ", "_") if not is_combined else "all"
    if repo_id is None:
        repo_id = repo_key

    for period_key, period_info in TIME_PERIODS.items():
        filtered_prs = filter_prs_by_period(prs, period_info["days"])
        if filtered_prs:
            overall_stats = calculate_overall_stats(filtered_prs)
            per_dev_stats = calculate_per_developer_stats(filtered_prs)
            trends = analyze_trends(filtered_prs)

            period_stats[period_key] = {
                "overall": overall_stats,
                "per_dev": per_dev_stats,
                "trends": {
                    "review_slope": trends.get("review_slope"),
                    "merge_slope": trends.get("merge_slope")
                },
                "name": period_info["name"],
                "pr_count": len(filtered_prs)
            }
            # Generate charts for this period
            period_charts[period_key] = create_period_visualizations(
                filtered_prs, period_key, period_info["name"], output_dir, repo_key
            )

    return {
        "repo_id": repo_id,
        "repo_name": repo_name,
        "is_combined": is_combined,
        "total_prs": len(prs),
        "period_stats": period_stats,
        "period_charts": period_charts
    }


def generate_json_data(prs_by_repo: Dict[str, List[Dict]], output_dir: str) -> str:
    """Generate JSON data file with all repository statistics and chart filenames."""
    from datetime import datetime, timezone

    report_time = datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M UTC")
    total_prs = sum(len(prs) for prs in prs_by_repo.values())

    # Build data structure
    data = {
        "meta": {
            "generated_at": report_time,
            "total_repositories": len(prs_by_repo),
            "total_prs": total_prs
        },
        "config": {
            "pr_size_small": PR_SIZE_SMALL,
            "pr_size_medium": PR_SIZE_MEDIUM
        },
        "time_periods": TIME_PERIODS,
        "repositories": [],
        "raw_prs": {}  # Store all PRs for client-side filtering
    }

    # Calculate global per-developer stats across all repositories
    if len(prs_by_repo) > 1:
        all_prs = []
        for prs in prs_by_repo.values():
            all_prs.extend(prs)

        global_dev_stats = calculate_per_developer_stats(all_prs)
        data["global_dev_stats"] = global_dev_stats

        # Generate "All Repositories Combined" data
        combined_data = generate_repo_data("All Repositories", all_prs, output_dir, is_combined=True, repo_id="overall")
        data["repositories"].append(combined_data)

    # Generate individual repository data and store raw PRs
    for repo_name in sorted(prs_by_repo.keys()):
        repo_data = generate_repo_data(repo_name, prs_by_repo[repo_name], output_dir, is_combined=False)
        data["repositories"].append(repo_data)

        # Store raw PR data for this repository (for client-side filtering)
        data["raw_prs"][repo_name] = prs_by_repo[repo_name]

    # Write JSON file
    json_path = os.path.join(output_dir, "report-data.json")
    with open(json_path, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=2, default=str)

    return json_path


def main() -> None:
    args = parse_args()

    # Determine which CSV files to analyze
    csv_files = []
    if args.input:
        # Single file specified
        csv_files = [args.input]
        print(f"🔍 Loading PR data from {args.input}...")
    else:
        # Auto-detect CSV files in data directory
        print(f"🔍 Scanning {args.data_dir} for CSV files...")
        csv_files = find_csv_files(args.data_dir)

        if not csv_files:
            print(f"❌ No CSV files found in {args.data_dir}", file=sys.stderr)
            print(f"   Run 'python gh_pr_times.py --repos owner/repo' first to fetch data", file=sys.stderr)
            sys.exit(1)

        print(f"✓ Found {len(csv_files)} CSV file(s):")
        for csv_file in csv_files:
            print(f"  - {os.path.basename(csv_file)}")

    # Load data by repository
    prs_by_repo = load_prs_by_repository(csv_files)

    if not prs_by_repo:
        print("❌ No PR data found in CSV file(s).", file=sys.stderr)
        sys.exit(1)

    total_prs = sum(len(prs) for prs in prs_by_repo.values())
    print(f"✓ Loaded {total_prs} PRs total from {len(prs_by_repo)} repository/repositories")

    print("\n📊 Generating charts and data...")
    if len(prs_by_repo) > 1:
        print(f"  └─ Creating combined view for all {len(prs_by_repo)} repositories")
    for repo_name in sorted(prs_by_repo.keys()):
        print(f"  └─ Processing {repo_name} ({len(prs_by_repo[repo_name])} PRs)")

    # Set matplotlib style
    plt.style.use(CHART_STYLE)

    # Generate JSON data file (includes all stats and references to charts)
    json_path = generate_json_data(prs_by_repo, args.output_dir)

    print(f"\n✅ Analysis complete!")
    print(f"   📊 Data: {json_path}")
    print(f"   📈 Charts: {args.output_dir}/")
    print(f"\n💡 Open index.html in your browser to view the report!")


if __name__ == "__main__":
    main()
