#!/usr/bin/env python3
"""
analyze_pr_times.py

Analyze and visualize PR review time metrics from CSV data generated by gh_pr_times.py

Features:
- Overall statistics (average review times, merge times)
- Per-developer statistics
- Trend analysis showing if review times are improving or degrading
- Visual charts and graphs
- Auto-detects all CSV files in data directory

Usage:
  # Analyze all CSV files in ./data directory (default)
  python analyze_pr_times.py

  # Analyze a specific CSV file
  python analyze_pr_times.py --input results.csv

  # Analyze all CSVs in custom directory
  python analyze_pr_times.py --data-dir ./my-data --output-dir ./analytics
"""

import argparse
import csv
import glob
import os
import sys
from collections import defaultdict
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional

import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import numpy as np
from scipy import stats


def parse_args() -> argparse.Namespace:
    p = argparse.ArgumentParser(description="Analyze PR review time metrics from CSV")
    p.add_argument("--input", type=str, default=None, help="Input CSV file (if not provided, analyzes all CSVs in --data-dir)")
    p.add_argument("--data-dir", type=str, default="./data", help="Directory to scan for CSV files when --input not provided")
    p.add_argument("--output-dir", type=str, default="./analytics", help="Output directory for charts")
    p.add_argument("--min-prs", type=int, default=3, help="Minimum PRs for per-developer stats")
    return p.parse_args()


def find_csv_files(data_dir: str) -> List[str]:
    """Find all CSV files in the data directory."""
    if not os.path.exists(data_dir):
        return []

    csv_files = glob.glob(os.path.join(data_dir, "*.csv"))
    return sorted(csv_files)


def load_pr_data(csv_path: str) -> List[Dict]:
    """Load PR data from CSV file."""
    prs = []
    try:
        with open(csv_path, "r", encoding="utf-8") as f:
            reader = csv.DictReader(f)
            for row in reader:
                prs.append(row)
    except FileNotFoundError:
        print(f"‚ùå Error: File '{csv_path}' not found.", file=sys.stderr)
        sys.exit(1)
    return prs


def load_multiple_csv_files(csv_paths: List[str]) -> List[Dict]:
    """Load PR data from multiple CSV files and combine them."""
    all_prs = []
    for csv_path in csv_paths:
        try:
            with open(csv_path, "r", encoding="utf-8") as f:
                reader = csv.DictReader(f)
                for row in reader:
                    all_prs.append(row)
        except Exception as e:
            print(f"‚ö†Ô∏è  Warning: Could not read {csv_path}: {e}", file=sys.stderr)
    return all_prs


def parse_float(value: str) -> Optional[float]:
    """Safely parse float value."""
    if not value or value == "":
        return None
    try:
        return float(value)
    except (ValueError, TypeError):
        return None


def parse_datetime(value: str) -> Optional[datetime]:
    """Safely parse datetime value."""
    if not value or value == "":
        return None
    try:
        return datetime.fromisoformat(value.replace("Z", "+00:00"))
    except (ValueError, TypeError):
        return None


def calculate_overall_stats(prs: List[Dict]) -> Dict:
    """Calculate overall statistics."""
    review_times = [parse_float(pr["time_to_first_review_hours"]) for pr in prs]
    review_times = [t for t in review_times if t is not None]

    merge_times = [parse_float(pr["time_to_merge_hours"]) for pr in prs]
    merge_times = [t for t in merge_times if t is not None]

    open_times = [parse_float(pr["open_time_hours"]) for pr in prs]
    open_times = [t for t in open_times if t is not None]

    total_prs = len(prs)
    merged_prs = len([pr for pr in prs if pr["merged_at"]])
    open_prs = len([pr for pr in prs if not pr["merged_at"]])

    return {
        "total_prs": total_prs,
        "merged_prs": merged_prs,
        "open_prs": open_prs,
        "avg_review_time": np.mean(review_times) if review_times else None,
        "median_review_time": np.median(review_times) if review_times else None,
        "avg_merge_time": np.mean(merge_times) if merge_times else None,
        "median_merge_time": np.median(merge_times) if merge_times else None,
        "avg_open_time": np.mean(open_times) if open_times else None,
    }


def calculate_per_developer_stats(prs: List[Dict], min_prs: int = 3) -> Dict[str, Dict]:
    """Calculate statistics per developer."""
    dev_data = defaultdict(lambda: {"review_times": [], "merge_times": [], "pr_count": 0})

    for pr in prs:
        author = pr.get("author", "unknown")
        if not author:
            continue

        dev_data[author]["pr_count"] += 1

        review_time = parse_float(pr["time_to_first_review_hours"])
        if review_time is not None:
            dev_data[author]["review_times"].append(review_time)

        merge_time = parse_float(pr["time_to_merge_hours"])
        if merge_time is not None:
            dev_data[author]["merge_times"].append(merge_time)

    # Calculate averages and filter by min_prs
    dev_stats = {}
    for author, data in dev_data.items():
        if data["pr_count"] >= min_prs:
            dev_stats[author] = {
                "pr_count": data["pr_count"],
                "avg_review_time": np.mean(data["review_times"]) if data["review_times"] else None,
                "avg_merge_time": np.mean(data["merge_times"]) if data["merge_times"] else None,
            }

    return dev_stats


def analyze_trends(prs: List[Dict]) -> Dict:
    """Analyze trends in review times over time."""
    # Sort PRs by creation date
    dated_prs = []
    for pr in prs:
        created = parse_datetime(pr["created_at"])
        review_time = parse_float(pr["time_to_first_review_hours"])
        merge_time = parse_float(pr["time_to_merge_hours"])
        if created:
            dated_prs.append({
                "date": created,
                "review_time": review_time,
                "merge_time": merge_time,
            })

    dated_prs.sort(key=lambda x: x["date"])

    if len(dated_prs) < 2:
        return {"trend": "insufficient_data"}

    # Calculate trend using linear regression
    dates_numeric = [(pr["date"] - dated_prs[0]["date"]).days for pr in dated_prs]

    # Review time trend
    review_data = [(dates_numeric[i], pr["review_time"])
                   for i, pr in enumerate(dated_prs) if pr["review_time"] is not None]

    review_slope = None
    if len(review_data) >= 2:
        x, y = zip(*review_data)
        slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)
        review_slope = slope

    # Merge time trend
    merge_data = [(dates_numeric[i], pr["merge_time"])
                  for i, pr in enumerate(dated_prs) if pr["merge_time"] is not None]

    merge_slope = None
    if len(merge_data) >= 2:
        x, y = zip(*merge_data)
        slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)
        merge_slope = slope

    return {
        "review_slope": review_slope,
        "merge_slope": merge_slope,
        "dated_prs": dated_prs,
    }


def print_statistics(overall: Dict, per_dev: Dict, trends: Dict) -> None:
    """Print statistics to console."""
    print("\n" + "=" * 80)
    print("üìä PR REVIEW TIME ANALYSIS")
    print("=" * 80)

    # Overall stats
    print("\nüìà OVERALL STATISTICS")
    print("-" * 80)
    print(f"Total PRs:              {overall['total_prs']}")
    print(f"  ‚îî‚îÄ Merged:            {overall['merged_prs']}")
    print(f"  ‚îî‚îÄ Open:              {overall['open_prs']}")

    if overall["avg_review_time"]:
        print(f"\nTime to First Review:")
        print(f"  ‚îî‚îÄ Average:           {overall['avg_review_time']:.2f} hours ({overall['avg_review_time']/24:.1f} days)")
        print(f"  ‚îî‚îÄ Median:            {overall['median_review_time']:.2f} hours ({overall['median_review_time']/24:.1f} days)")

    if overall["avg_merge_time"]:
        print(f"\nTime to Merge:")
        print(f"  ‚îî‚îÄ Average:           {overall['avg_merge_time']:.2f} hours ({overall['avg_merge_time']/24:.1f} days)")
        print(f"  ‚îî‚îÄ Median:            {overall['median_merge_time']:.2f} hours ({overall['median_merge_time']/24:.1f} days)")

    # Trends
    print("\nüìâ TRENDS")
    print("-" * 80)
    if trends.get("review_slope") is not None:
        review_direction = "üî¥ INCREASING" if trends["review_slope"] > 0 else "üü¢ DECREASING"
        days_change = abs(trends["review_slope"]) * 30  # Change per 30 days
        print(f"Review Time Trend:      {review_direction}")
        print(f"  ‚îî‚îÄ Change per month:  {days_change:.2f} hours")

    if trends.get("merge_slope") is not None:
        merge_direction = "üî¥ INCREASING" if trends["merge_slope"] > 0 else "üü¢ DECREASING"
        days_change = abs(trends["merge_slope"]) * 30
        print(f"Merge Time Trend:       {merge_direction}")
        print(f"  ‚îî‚îÄ Change per month:  {days_change:.2f} hours")

    # Per developer
    if per_dev:
        print("\nüë• PER DEVELOPER STATISTICS")
        print("-" * 80)
        # Sort by PR count
        sorted_devs = sorted(per_dev.items(), key=lambda x: x[1]["pr_count"], reverse=True)

        for author, stats in sorted_devs:
            print(f"\n{author}:")
            print(f"  ‚îî‚îÄ PRs:               {stats['pr_count']}")
            if stats["avg_review_time"] is not None:
                print(f"  ‚îî‚îÄ Avg Review Time:   {stats['avg_review_time']:.2f} hours ({stats['avg_review_time']/24:.1f} days)")
            if stats["avg_merge_time"] is not None:
                print(f"  ‚îî‚îÄ Avg Merge Time:    {stats['avg_merge_time']:.2f} hours ({stats['avg_merge_time']/24:.1f} days)")

    print("\n" + "=" * 80)


def create_visualizations(prs: List[Dict], overall: Dict, per_dev: Dict, trends: Dict, output_dir: str) -> None:
    """Create visualization charts."""
    Path(output_dir).mkdir(parents=True, exist_ok=True)

    # Set style
    plt.style.use('seaborn-v0_8-darkgrid')

    # 1. Review Time Trend Over Time
    dated_prs = trends.get("dated_prs", [])
    if dated_prs:
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))

        # Review times
        dates = [pr["date"] for pr in dated_prs if pr["review_time"] is not None]
        review_times = [pr["review_time"] for pr in dated_prs if pr["review_time"] is not None]

        if dates and review_times:
            ax1.scatter(dates, review_times, alpha=0.6, s=30)

            # Add trend line
            if trends.get("review_slope") is not None:
                dates_numeric = [(d - dates[0]).days for d in dates]
                slope = trends["review_slope"]
                intercept = np.mean(review_times) - slope * np.mean(dates_numeric)
                trend_line = [slope * d + intercept for d in dates_numeric]
                ax1.plot(dates, trend_line, 'r--', linewidth=2, label=f'Trend: {slope*30:.2f} hrs/month')

            ax1.set_ylabel('Hours', fontsize=12)
            ax1.set_title('Time to First Review Over Time', fontsize=14, fontweight='bold')
            ax1.legend()
            ax1.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))
            ax1.xaxis.set_major_locator(mdates.AutoDateLocator())
            plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45, ha='right')

        # Merge times
        merge_dates = [pr["date"] for pr in dated_prs if pr["merge_time"] is not None]
        merge_times = [pr["merge_time"] for pr in dated_prs if pr["merge_time"] is not None]

        if merge_dates and merge_times:
            ax2.scatter(merge_dates, merge_times, alpha=0.6, s=30, color='green')

            # Add trend line
            if trends.get("merge_slope") is not None:
                dates_numeric = [(d - merge_dates[0]).days for d in merge_dates]
                slope = trends["merge_slope"]
                intercept = np.mean(merge_times) - slope * np.mean(dates_numeric)
                trend_line = [slope * d + intercept for d in dates_numeric]
                ax2.plot(merge_dates, trend_line, 'r--', linewidth=2, label=f'Trend: {slope*30:.2f} hrs/month')

            ax2.set_xlabel('Date', fontsize=12)
            ax2.set_ylabel('Hours', fontsize=12)
            ax2.set_title('Time to Merge Over Time', fontsize=14, fontweight='bold')
            ax2.legend()
            ax2.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))
            ax2.xaxis.set_major_locator(mdates.AutoDateLocator())
            plt.setp(ax2.xaxis.get_majorticklabels(), rotation=45, ha='right')

        plt.tight_layout()
        plt.savefig(f"{output_dir}/trends_over_time.png", dpi=150, bbox_inches='tight')
        print(f"‚úì Saved: {output_dir}/trends_over_time.png")
        plt.close()

    # 2. Per Developer Comparison
    if per_dev:
        sorted_devs = sorted(per_dev.items(), key=lambda x: x[1]["pr_count"], reverse=True)
        authors = [author for author, _ in sorted_devs]
        review_times = [stats["avg_review_time"] or 0 for _, stats in sorted_devs]
        merge_times = [stats["avg_merge_time"] or 0 for _, stats in sorted_devs]

        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))

        # Review times
        ax1.barh(authors, review_times, color='skyblue')
        ax1.set_xlabel('Average Hours to First Review', fontsize=12)
        ax1.set_title('Average Review Time by Developer', fontsize=14, fontweight='bold')
        ax1.invert_yaxis()

        # Merge times
        ax2.barh(authors, merge_times, color='lightgreen')
        ax2.set_xlabel('Average Hours to Merge', fontsize=12)
        ax2.set_title('Average Merge Time by Developer', fontsize=14, fontweight='bold')
        ax2.invert_yaxis()

        plt.tight_layout()
        plt.savefig(f"{output_dir}/per_developer_stats.png", dpi=150, bbox_inches='tight')
        print(f"‚úì Saved: {output_dir}/per_developer_stats.png")
        plt.close()

    # 3. Distribution Histograms
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

    review_times = [parse_float(pr["time_to_first_review_hours"]) for pr in prs]
    review_times = [t for t in review_times if t is not None and t < 200]  # Cap outliers

    if review_times:
        ax1.hist(review_times, bins=30, color='skyblue', edgecolor='black', alpha=0.7)
        ax1.axvline(np.mean(review_times), color='red', linestyle='--', linewidth=2, label=f'Mean: {np.mean(review_times):.1f}h')
        ax1.axvline(np.median(review_times), color='green', linestyle='--', linewidth=2, label=f'Median: {np.median(review_times):.1f}h')
        ax1.set_xlabel('Hours', fontsize=12)
        ax1.set_ylabel('Frequency', fontsize=12)
        ax1.set_title('Distribution of Review Times', fontsize=14, fontweight='bold')
        ax1.legend()

    merge_times = [parse_float(pr["time_to_merge_hours"]) for pr in prs]
    merge_times = [t for t in merge_times if t is not None and t < 500]  # Cap outliers

    if merge_times:
        ax2.hist(merge_times, bins=30, color='lightgreen', edgecolor='black', alpha=0.7)
        ax2.axvline(np.mean(merge_times), color='red', linestyle='--', linewidth=2, label=f'Mean: {np.mean(merge_times):.1f}h')
        ax2.axvline(np.median(merge_times), color='green', linestyle='--', linewidth=2, label=f'Median: {np.median(merge_times):.1f}h')
        ax2.set_xlabel('Hours', fontsize=12)
        ax2.set_ylabel('Frequency', fontsize=12)
        ax2.set_title('Distribution of Merge Times', fontsize=14, fontweight='bold')
        ax2.legend()

    plt.tight_layout()
    plt.savefig(f"{output_dir}/distributions.png", dpi=150, bbox_inches='tight')
    print(f"‚úì Saved: {output_dir}/distributions.png")
    plt.close()


def main() -> None:
    args = parse_args()

    # Determine which CSV files to analyze
    csv_files = []
    if args.input:
        # Single file specified
        csv_files = [args.input]
        print(f"üîç Loading PR data from {args.input}...")
    else:
        # Auto-detect CSV files in data directory
        print(f"üîç Scanning {args.data_dir} for CSV files...")
        csv_files = find_csv_files(args.data_dir)

        if not csv_files:
            print(f"‚ùå No CSV files found in {args.data_dir}", file=sys.stderr)
            print(f"   Run 'python gh_pr_times.py --repos owner/repo' first to fetch data", file=sys.stderr)
            sys.exit(1)

        print(f"‚úì Found {len(csv_files)} CSV file(s):")
        for csv_file in csv_files:
            print(f"  - {os.path.basename(csv_file)}")

    # Load data from CSV file(s)
    if len(csv_files) == 1:
        prs = load_pr_data(csv_files[0])
    else:
        print(f"\nüìä Combining data from {len(csv_files)} repositories...")
        prs = load_multiple_csv_files(csv_files)

    if not prs:
        print("‚ùå No PR data found in CSV file(s).", file=sys.stderr)
        sys.exit(1)

    print(f"‚úì Loaded {len(prs)} PRs total")

    print("\nüìä Calculating statistics...")
    overall = calculate_overall_stats(prs)
    per_dev = calculate_per_developer_stats(prs, args.min_prs)
    trends = analyze_trends(prs)

    print_statistics(overall, per_dev, trends)

    print("\nüìà Creating visualizations...")
    create_visualizations(prs, overall, per_dev, trends, args.output_dir)

    print(f"\n‚úÖ Analysis complete! Charts saved to '{args.output_dir}/'")


if __name__ == "__main__":
    main()

