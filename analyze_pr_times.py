#!/usr/bin/env python3
"""
analyze_pr_times.py

Analyze and visualize PR review time metrics from CSV data generated by gh_pr_times.py

Features:
- Overall statistics (average review times, merge times)
- Per-developer statistics
- Trend analysis showing if review times are improving or degrading
- Visual charts and graphs
- Auto-detects all CSV files in data directory

Usage:
  # Analyze all CSV files in ./data directory (default)
  python analyze_pr_times.py

  # Analyze a specific CSV file
  python analyze_pr_times.py --input results.csv

  # Analyze all CSVs in custom directory
  python analyze_pr_times.py --data-dir ./my-data --output-dir ./analytics
"""

import argparse
import csv
import glob
import os
import sys
from collections import defaultdict
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional

import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import numpy as np
from scipy import stats

# Constants
HOURS_PER_DAY = 24
DAYS_PER_MONTH = 30
MIN_PRS_DEFAULT = 3
CHART_DPI = 150
CHART_STYLE = 'seaborn-v0_8-darkgrid'
OUTLIER_CAP_REVIEW_HOURS = 200
OUTLIER_CAP_MERGE_HOURS = 500

# Time periods for analysis
TIME_PERIODS = {
    "last_30_days": {"name": "Last 30 Days", "days": 30},
    "last_quarter": {"name": "Last Quarter", "days": 90},
    "overall": {"name": "Overall", "days": None}
}


def parse_args() -> argparse.Namespace:
    p = argparse.ArgumentParser(description="Analyze PR review time metrics from CSV")
    p.add_argument("--input", type=str, default=None, help="Input CSV file (if not provided, analyzes all CSVs in --data-dir)")
    p.add_argument("--data-dir", type=str, default="./data", help="Directory to scan for CSV files when --input not provided")
    p.add_argument("--output-dir", type=str, default="./analytics", help="Output directory for charts")
    p.add_argument("--min-prs", type=int, default=MIN_PRS_DEFAULT, help="Minimum PRs for per-developer stats")
    return p.parse_args()


def find_csv_files(data_dir: str) -> List[str]:
    """Find all CSV files in the data directory."""
    if not os.path.exists(data_dir):
        return []

    csv_files = glob.glob(os.path.join(data_dir, "*.csv"))
    return sorted(csv_files)


def load_pr_data(csv_path: str) -> List[Dict]:
    """Load PR data from CSV file."""
    prs = []
    try:
        with open(csv_path, "r", encoding="utf-8") as f:
            reader = csv.DictReader(f)
            for row in reader:
                prs.append(row)
    except FileNotFoundError:
        print(f"❌ Error: File '{csv_path}' not found.", file=sys.stderr)
        sys.exit(1)
    return prs


def load_prs_by_repository(csv_paths: List[str]) -> Dict[str, List[Dict]]:
    """Load PR data organized by repository."""
    prs_by_repo = {}

    for csv_path in csv_paths:
        try:
            prs = load_pr_data(csv_path)
            if prs:
                # Get repository name from first PR's repo field, or from filename
                repo_name = prs[0].get("repo", os.path.basename(csv_path).replace(".csv", "").replace("_", "/"))
                prs_by_repo[repo_name] = prs
        except SystemExit:
            # load_pr_data calls sys.exit on FileNotFoundError, handle gracefully for multiple files
            print(f"⚠️  Warning: Could not read {csv_path}", file=sys.stderr)

    return prs_by_repo


def load_multiple_csv_files(csv_paths: List[str]) -> List[Dict]:
    """Load PR data from multiple CSV files and combine them."""
    all_prs = []
    for csv_path in csv_paths:
        try:
            prs = load_pr_data(csv_path)
            all_prs.extend(prs)
        except SystemExit:
            # load_pr_data calls sys.exit on FileNotFoundError, handle gracefully for multiple files
            print(f"⚠️  Warning: Could not read {csv_path}", file=sys.stderr)
    return all_prs


def parse_float(value: str) -> Optional[float]:
    """Safely parse float value."""
    if not value or value == "":
        return None
    try:
        return float(value)
    except (ValueError, TypeError):
        return None


def parse_datetime(value: str) -> Optional[datetime]:
    """Safely parse datetime value."""
    if not value or value == "":
        return None
    try:
        return datetime.fromisoformat(value.replace("Z", "+00:00"))
    except (ValueError, TypeError):
        return None


def hours_to_days(hours: float) -> float:
    """Convert hours to days."""
    return hours / HOURS_PER_DAY


def filter_prs_by_period(prs: List[Dict], days: Optional[int] = None) -> List[Dict]:
    """Filter PRs by time period (last N days). If days is None, return all PRs."""
    if days is None:
        return prs

    from datetime import timedelta, timezone
    cutoff_date = datetime.now(timezone.utc) - timedelta(days=days)

    filtered = []
    for pr in prs:
        created = parse_datetime(pr["created_at"])
        if created and created >= cutoff_date:
            filtered.append(pr)

    return filtered


def calculate_overall_stats(prs: List[Dict]) -> Dict:
    """Calculate overall statistics."""
    review_times = [parse_float(pr["time_to_first_review_hours"]) for pr in prs]
    review_times = [t for t in review_times if t is not None]

    merge_times = [parse_float(pr["time_to_merge_hours"]) for pr in prs]
    merge_times = [t for t in merge_times if t is not None]

    open_times = [parse_float(pr["open_time_hours"]) for pr in prs]
    open_times = [t for t in open_times if t is not None]

    total_prs = len(prs)
    merged_prs = len([pr for pr in prs if pr["merged_at"]])
    closed_not_merged_prs = len([pr for pr in prs if pr["closed_at"] and not pr["merged_at"]])
    open_prs = len([pr for pr in prs if not pr["closed_at"]])

    return {
        "total_prs": total_prs,
        "merged_prs": merged_prs,
        "closed_not_merged_prs": closed_not_merged_prs,
        "open_prs": open_prs,
        "avg_review_time": np.mean(review_times) if review_times else None,
        "median_review_time": np.median(review_times) if review_times else None,
        "avg_merge_time": np.mean(merge_times) if merge_times else None,
        "median_merge_time": np.median(merge_times) if merge_times else None,
        "avg_open_time": np.mean(open_times) if open_times else None,
    }


def calculate_per_developer_stats(prs: List[Dict], min_prs: int = 3) -> Dict[str, Dict]:
    """Calculate statistics per developer."""
    dev_data = defaultdict(lambda: {"review_times": [], "merge_times": [], "pr_count": 0})

    for pr in prs:
        author = pr.get("author", "unknown")
        if not author:
            continue

        dev_data[author]["pr_count"] += 1

        review_time = parse_float(pr["time_to_first_review_hours"])
        if review_time is not None:
            dev_data[author]["review_times"].append(review_time)

        merge_time = parse_float(pr["time_to_merge_hours"])
        if merge_time is not None:
            dev_data[author]["merge_times"].append(merge_time)

    # Calculate averages and filter by min_prs
    dev_stats = {}
    for author, data in dev_data.items():
        if data["pr_count"] >= min_prs:
            dev_stats[author] = {
                "pr_count": data["pr_count"],
                "avg_review_time": np.mean(data["review_times"]) if data["review_times"] else None,
                "avg_merge_time": np.mean(data["merge_times"]) if data["merge_times"] else None,
            }

    return dev_stats


def analyze_trends(prs: List[Dict]) -> Dict:
    """Analyze trends in review times over time."""
    # Sort PRs by creation date
    dated_prs = []
    for pr in prs:
        created = parse_datetime(pr["created_at"])
        review_time = parse_float(pr["time_to_first_review_hours"])
        merge_time = parse_float(pr["time_to_merge_hours"])
        if created:
            dated_prs.append({
                "date": created,
                "review_time": review_time,
                "merge_time": merge_time,
            })

    dated_prs.sort(key=lambda x: x["date"])

    if len(dated_prs) < 2:
        return {"trend": "insufficient_data"}

    # Calculate trend using linear regression
    dates_numeric = [(pr["date"] - dated_prs[0]["date"]).days for pr in dated_prs]

    # Review time trend
    review_data = [(dates_numeric[i], pr["review_time"])
                   for i, pr in enumerate(dated_prs) if pr["review_time"] is not None]

    review_slope = None
    if len(review_data) >= 2:
        x, y = zip(*review_data)
        slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)
        review_slope = slope

    # Merge time trend
    merge_data = [(dates_numeric[i], pr["merge_time"])
                  for i, pr in enumerate(dated_prs) if pr["merge_time"] is not None]

    merge_slope = None
    if len(merge_data) >= 2:
        x, y = zip(*merge_data)
        slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)
        merge_slope = slope

    return {
        "review_slope": review_slope,
        "merge_slope": merge_slope,
        "dated_prs": dated_prs,
    }


def print_time_period_stats(prs: List[Dict], period_name: str, period_days: Optional[int] = None) -> None:
    """Print statistics for a specific time period."""
    filtered_prs = filter_prs_by_period(prs, period_days)

    if not filtered_prs:
        print(f"\n📊 {period_name.upper()} - No PRs in this period")
        return

    stats = calculate_overall_stats(filtered_prs)

    print(f"\n📊 {period_name.upper()}")
    print("-" * 80)
    print(f"PRs in period:          {stats['total_prs']}")
    print(f"  └─ Merged:            {stats['merged_prs']}")
    print(f"  └─ Closed (not merged): {stats['closed_not_merged_prs']}")
    print(f"  └─ Still Open:        {stats['open_prs']}")

    if stats["avg_review_time"]:
        print(f"\nTime to First Review:   {stats['avg_review_time']:.2f} hrs ({hours_to_days(stats['avg_review_time']):.1f} days) avg, {stats['median_review_time']:.2f} hrs median")

    if stats["avg_merge_time"]:
        print(f"Time to Merge:          {stats['avg_merge_time']:.2f} hrs ({hours_to_days(stats['avg_merge_time']):.1f} days) avg, {stats['median_merge_time']:.2f} hrs median")
        if stats["avg_review_time"]:
            review_to_merge = stats['avg_merge_time'] - stats['avg_review_time']
            print(f"Review → Merge:         {review_to_merge:.2f} hrs ({hours_to_days(review_to_merge):.1f} days)")


def print_statistics(overall: Dict, per_dev: Dict, trends: Dict) -> None:
    """Print statistics to console."""
    print("\n" + "=" * 80)
    print("📊 PR REVIEW TIME ANALYSIS")
    print("=" * 80)

    # Overall stats
    print("\n📈 OVERALL STATISTICS")
    print("-" * 80)
    print(f"Total PRs:              {overall['total_prs']}")
    print(f"  └─ Merged:            {overall['merged_prs']}")
    print(f"  └─ Closed (not merged): {overall['closed_not_merged_prs']}")
    print(f"  └─ Still Open:        {overall['open_prs']}")

    if overall["avg_review_time"]:
        print(f"\nTime to First Review (creation → first review):")
        print(f"  └─ Average:           {overall['avg_review_time']:.2f} hours ({hours_to_days(overall['avg_review_time']):.1f} days)")
        print(f"  └─ Median:            {overall['median_review_time']:.2f} hours ({hours_to_days(overall['median_review_time']):.1f} days)")

    if overall["avg_merge_time"]:
        print(f"\nTime to Merge (creation → merge):")
        print(f"  └─ Average:           {overall['avg_merge_time']:.2f} hours ({hours_to_days(overall['avg_merge_time']):.1f} days)")
        print(f"  └─ Median:            {overall['median_merge_time']:.2f} hours ({hours_to_days(overall['median_merge_time']):.1f} days)")

        if overall["avg_review_time"]:
            review_to_merge = overall['avg_merge_time'] - overall['avg_review_time']
            print(f"  └─ Avg time from first review to merge: {review_to_merge:.2f} hours ({hours_to_days(review_to_merge):.1f} days)")

    # Trends
    print("\n📉 TRENDS")
    print("-" * 80)
    if trends.get("review_slope") is not None:
        review_direction = "🔴 INCREASING" if trends["review_slope"] > 0 else "🟢 DECREASING"
        days_change = abs(trends["review_slope"]) * DAYS_PER_MONTH
        print(f"Review Time Trend:      {review_direction}")
        print(f"  └─ Change per month:  {days_change:.2f} hours")

    if trends.get("merge_slope") is not None:
        merge_direction = "🔴 INCREASING" if trends["merge_slope"] > 0 else "🟢 DECREASING"
        days_change = abs(trends["merge_slope"]) * DAYS_PER_MONTH
        print(f"Merge Time Trend:       {merge_direction}")
        print(f"  └─ Change per month:  {days_change:.2f} hours")

    # Per developer
    if per_dev:
        print("\n👥 PER DEVELOPER STATISTICS")
        print("-" * 80)
        # Sort by PR count
        sorted_devs = sorted(per_dev.items(), key=lambda x: x[1]["pr_count"], reverse=True)

        for author, stats in sorted_devs:
            print(f"\n{author}:")
            print(f"  └─ PRs:               {stats['pr_count']}")
            if stats["avg_review_time"] is not None:
                print(f"  └─ Avg Time to 1st Review: {stats['avg_review_time']:.2f} hours ({hours_to_days(stats['avg_review_time']):.1f} days)")
            if stats["avg_merge_time"] is not None:
                print(f"  └─ Avg Time to Merge: {stats['avg_merge_time']:.2f} hours ({hours_to_days(stats['avg_merge_time']):.1f} days)")
                if stats["avg_review_time"] is not None:
                    review_to_merge = stats['avg_merge_time'] - stats['avg_review_time']
                    print(f"  └─ Avg Review → Merge: {review_to_merge:.2f} hours ({hours_to_days(review_to_merge):.1f} days)")

    print("\n" + "=" * 80)


def _format_time_axis(ax):
    """Format axis for time-based charts."""
    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))
    ax.xaxis.set_major_locator(mdates.AutoDateLocator())
    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')


def _add_trend_line(ax, dates, values, slope):
    """Add trend line to a scatter plot."""
    dates_numeric = [(d - dates[0]).days for d in dates]
    intercept = np.mean(values) - slope * np.mean(dates_numeric)
    trend_line = [slope * d + intercept for d in dates_numeric]
    ax.plot(dates, trend_line, 'r--', linewidth=2, label=f'Trend: {slope*DAYS_PER_MONTH:.2f} hrs/month')


def _create_histogram(ax, values, color, title, outlier_cap):
    """Create a histogram with mean and median lines."""
    filtered_values = [v for v in values if v is not None and v < outlier_cap]

    if filtered_values:
        ax.hist(filtered_values, bins=30, color=color, edgecolor='black', alpha=0.7)
        ax.axvline(np.mean(filtered_values), color='red', linestyle='--', linewidth=2, label=f'Mean: {np.mean(filtered_values):.1f}h')
        ax.axvline(np.median(filtered_values), color='green', linestyle='--', linewidth=2, label=f'Median: {np.median(filtered_values):.1f}h')
        ax.set_xlabel('Hours', fontsize=12)
        ax.set_ylabel('Frequency', fontsize=12)
        ax.set_title(title, fontsize=14, fontweight='bold')
        ax.legend()


def create_period_visualizations(prs: List[Dict], period_key: str, period_name: str, output_dir: str, repo_key: str = "") -> Dict[str, str]:
    """Create visualization charts for a specific time period and repository, return chart filenames."""
    # Ensure output directory exists
    Path(output_dir).mkdir(parents=True, exist_ok=True)
    
    chart_files = {}

    # Create safe filename prefix for repo
    repo_prefix = repo_key.replace("/", "_").replace(" ", "_") + "_" if repo_key else ""

    # 1. Trends Over Time
    overall = calculate_overall_stats(prs)
    trends = analyze_trends(prs)
    dated_prs = trends.get("dated_prs", [])

    if dated_prs:
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))

        # Review times
        dates = [pr["date"] for pr in dated_prs if pr["review_time"] is not None]
        review_times = [pr["review_time"] for pr in dated_prs if pr["review_time"] is not None]

        if dates and review_times:
            ax1.scatter(dates, review_times, alpha=0.6, s=30)
            if trends.get("review_slope") is not None:
                _add_trend_line(ax1, dates, review_times, trends["review_slope"])
            ax1.set_ylabel('Hours', fontsize=12)
            title_suffix = f" - {period_name}" if period_name else ""
            ax1.set_title(f'Time to First Review{title_suffix}', fontsize=14, fontweight='bold')
            ax1.legend()
            _format_time_axis(ax1)

        # Merge times
        merge_dates = [pr["date"] for pr in dated_prs if pr["merge_time"] is not None]
        merge_times = [pr["merge_time"] for pr in dated_prs if pr["merge_time"] is not None]

        if merge_dates and merge_times:
            ax2.scatter(merge_dates, merge_times, alpha=0.6, s=30, color='green')
            if trends.get("merge_slope") is not None:
                _add_trend_line(ax2, merge_dates, merge_times, trends["merge_slope"])
            ax2.set_xlabel('Date', fontsize=12)
            ax2.set_ylabel('Hours', fontsize=12)
            ax2.set_title(f'Time to Merge{title_suffix}', fontsize=14, fontweight='bold')
            ax2.legend()
            _format_time_axis(ax2)

        plt.tight_layout()
        filename = f"{repo_prefix}trends_{period_key}.png"
        filepath = f"{output_dir}/{filename}"
        plt.savefig(filepath, dpi=CHART_DPI, bbox_inches='tight')
        chart_files['trends'] = filename
        plt.close()

    # 2. Distribution Histograms
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

    review_times = [parse_float(pr["time_to_first_review_hours"]) for pr in prs]
    _create_histogram(ax1, review_times, 'skyblue', f'Review Times Distribution{title_suffix if dated_prs else ""}', OUTLIER_CAP_REVIEW_HOURS)

    merge_times = [parse_float(pr["time_to_merge_hours"]) for pr in prs]
    _create_histogram(ax2, merge_times, 'lightgreen', f'Merge Times Distribution{title_suffix if dated_prs else ""}', OUTLIER_CAP_MERGE_HOURS)

    plt.tight_layout()
    filename = f"{repo_prefix}distributions_{period_key}.png"
    filepath = f"{output_dir}/{filename}"
    plt.savefig(filepath, dpi=CHART_DPI, bbox_inches='tight')
    chart_files['distributions'] = filename
    plt.close()

    return chart_files


def create_visualizations(prs: List[Dict], overall: Dict, per_dev: Dict, trends: Dict, output_dir: str) -> None:
    """Create visualization charts."""
    Path(output_dir).mkdir(parents=True, exist_ok=True)

    # Set style
    plt.style.use(CHART_STYLE)

    # 1. Review Time Trend Over Time
    dated_prs = trends.get("dated_prs", [])
    if dated_prs:
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))

        # Review times
        dates = [pr["date"] for pr in dated_prs if pr["review_time"] is not None]
        review_times = [pr["review_time"] for pr in dated_prs if pr["review_time"] is not None]

        if dates and review_times:
            ax1.scatter(dates, review_times, alpha=0.6, s=30)

            # Add trend line
            if trends.get("review_slope") is not None:
                _add_trend_line(ax1, dates, review_times, trends["review_slope"])

            ax1.set_ylabel('Hours', fontsize=12)
            ax1.set_title('Time to First Review Over Time', fontsize=14, fontweight='bold')
            ax1.legend()
            _format_time_axis(ax1)

        # Merge times
        merge_dates = [pr["date"] for pr in dated_prs if pr["merge_time"] is not None]
        merge_times = [pr["merge_time"] for pr in dated_prs if pr["merge_time"] is not None]

        if merge_dates and merge_times:
            ax2.scatter(merge_dates, merge_times, alpha=0.6, s=30, color='green')

            # Add trend line
            if trends.get("merge_slope") is not None:
                _add_trend_line(ax2, merge_dates, merge_times, trends["merge_slope"])

            ax2.set_xlabel('Date', fontsize=12)
            ax2.set_ylabel('Hours', fontsize=12)
            ax2.set_title('Time to Merge Over Time', fontsize=14, fontweight='bold')
            ax2.legend()
            _format_time_axis(ax2)

        plt.tight_layout()
        plt.savefig(f"{output_dir}/trends_over_time.png", dpi=CHART_DPI, bbox_inches='tight')
        print(f"✓ Saved: {output_dir}/trends_over_time.png")
        plt.close()

    # 2. Per Developer Comparison
    if per_dev:
        sorted_devs = sorted(per_dev.items(), key=lambda x: x[1]["pr_count"], reverse=True)
        authors = [author for author, _ in sorted_devs]
        review_times = [stats["avg_review_time"] or 0 for _, stats in sorted_devs]
        merge_times = [stats["avg_merge_time"] or 0 for _, stats in sorted_devs]

        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))

        # Review times
        ax1.barh(authors, review_times, color='skyblue')
        ax1.set_xlabel('Average Hours to First Review', fontsize=12)
        ax1.set_title('Average Review Time by Developer', fontsize=14, fontweight='bold')
        ax1.invert_yaxis()

        # Merge times
        ax2.barh(authors, merge_times, color='lightgreen')
        ax2.set_xlabel('Average Hours to Merge', fontsize=12)
        ax2.set_title('Average Merge Time by Developer', fontsize=14, fontweight='bold')
        ax2.invert_yaxis()

        plt.tight_layout()
        plt.savefig(f"{output_dir}/per_developer_stats.png", dpi=CHART_DPI, bbox_inches='tight')
        print(f"✓ Saved: {output_dir}/per_developer_stats.png")
        plt.close()

    # 3. Distribution Histograms
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

    review_times = [parse_float(pr["time_to_first_review_hours"]) for pr in prs]
    _create_histogram(ax1, review_times, 'skyblue', 'Distribution of Review Times', OUTLIER_CAP_REVIEW_HOURS)

    merge_times = [parse_float(pr["time_to_merge_hours"]) for pr in prs]
    _create_histogram(ax2, merge_times, 'lightgreen', 'Distribution of Merge Times', OUTLIER_CAP_MERGE_HOURS)

    plt.tight_layout()
    plt.savefig(f"{output_dir}/distributions.png", dpi=CHART_DPI, bbox_inches='tight')
    print(f"✓ Saved: {output_dir}/distributions.png")
    plt.close()


def generate_repo_section(repo_name: str, prs: List[Dict], output_dir: str, is_combined: bool = False, repo_index: int = 0) -> str:
    """Generate HTML section for a single repository or combined view."""
    # Calculate stats for all time periods
    period_stats = {}
    period_charts = {}

    # Create safe repo key for filenames
    repo_key = repo_name.replace("/", "_").replace(" ", "_") if not is_combined else "all"

    for period_key, period_info in TIME_PERIODS.items():
        filtered_prs = filter_prs_by_period(prs, period_info["days"])
        if filtered_prs:
            period_stats[period_key] = {
                "overall": calculate_overall_stats(filtered_prs),
                "per_dev": calculate_per_developer_stats(filtered_prs),
                "trends": analyze_trends(filtered_prs),
                "name": period_info["name"],
                "pr_count": len(filtered_prs)
            }
            # Generate charts for this period
            period_charts[period_key] = create_period_visualizations(
                filtered_prs, period_key, period_info["name"], output_dir, repo_key
            )

    # Generate per-developer chart for this repo
    per_dev = calculate_per_developer_stats(prs)
    dev_chart_file = None
    if per_dev:
        sorted_devs = sorted(per_dev.items(), key=lambda x: x[1]["pr_count"], reverse=True)
        authors = [author for author, _ in sorted_devs]
        review_times = [stats["avg_review_time"] or 0 for _, stats in sorted_devs]
        merge_times = [stats["avg_merge_time"] or 0 for _, stats in sorted_devs]

        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))
        ax1.barh(authors, review_times, color='skyblue')
        ax1.set_xlabel('Average Hours to First Review', fontsize=12)
        ax1.set_title('Average Review Time by Developer', fontsize=14, fontweight='bold')
        ax1.invert_yaxis()
        ax2.barh(authors, merge_times, color='lightgreen')
        ax2.set_xlabel('Average Hours to Merge', fontsize=12)
        ax2.set_title('Average Merge Time by Developer', fontsize=14, fontweight='bold')
        ax2.invert_yaxis()
        plt.tight_layout()
        dev_chart_file = f"{repo_key}_per_developer_stats.png"
        plt.savefig(f"{output_dir}/{dev_chart_file}", dpi=CHART_DPI, bbox_inches='tight')
        plt.close()

    # Build HTML section with accordion
    section_html = f"""
        <div class="repo-section" id="repo-{repo_index}">
            <div class="repo-header {'combined' if is_combined else ''}">
                <div>
                    <div class="repo-title">{"📊 All Repositories Combined" if is_combined else f"📁 {repo_name}"}</div>
                    <div class="repo-meta">Total PRs: {len(prs)}</div>
                </div>
                <div class="accordion-icon">▼</div>
            </div>
            <div class="repo-content">
"""

    # Add time period sections
    for period_key in ["overall", "last_quarter", "last_30_days"]:
        if period_key not in period_stats:
            continue

        stats = period_stats[period_key]
        overall = stats["overall"]
        trends = stats["trends"]
        name = stats["name"]

        section_html += f"""
        <div class="period-section">
            <h2>{name}</h2>

            <div class="stats-grid">
                <div class="stat-card">
                    <div class="stat-label">Total PRs</div>
                    <div class="stat-value">{overall['total_prs']}</div>
                </div>
                <div class="stat-card success">
                    <div class="stat-label">Merged</div>
                    <div class="stat-value">{overall['merged_prs']}</div>
                    <div class="stat-sub">{(overall['merged_prs']/overall['total_prs']*100):.1f}% merge rate</div>
                </div>
                <div class="stat-card warning">
                    <div class="stat-label">Closed (not merged)</div>
                    <div class="stat-value">{overall['closed_not_merged_prs']}</div>
                </div>
                <div class="stat-card info">
                    <div class="stat-label">Still Open</div>
                    <div class="stat-value">{overall['open_prs']}</div>
                </div>
            </div>
"""

        if overall.get("avg_review_time"):
            section_html += f"""
            <h3>⏱️ Time Metrics</h3>
            <div class="stats-grid">
                <div class="stat-card">
                    <div class="stat-label">Time to First Review</div>
                    <div class="stat-value">{overall['avg_review_time']:.1f}h</div>
                    <div class="stat-sub">Avg: {hours_to_days(overall['avg_review_time']):.1f} days | Median: {overall['median_review_time']:.1f}h</div>
                </div>
                <div class="stat-card">
                    <div class="stat-label">Time to Merge</div>
                    <div class="stat-value">{overall['avg_merge_time']:.1f}h</div>
                    <div class="stat-sub">Avg: {hours_to_days(overall['avg_merge_time']):.1f} days | Median: {overall['median_merge_time']:.1f}h</div>
                </div>
                <div class="stat-card">
                    <div class="stat-label">Review → Merge Time</div>
                    <div class="stat-value">{(overall['avg_merge_time'] - overall['avg_review_time']):.1f}h</div>
                    <div class="stat-sub">{hours_to_days(overall['avg_merge_time'] - overall['avg_review_time']):.1f} days avg</div>
                </div>
            </div>
"""

        # Trends
        if trends.get("review_slope") is not None or trends.get("merge_slope") is not None:
            section_html += f"""
            <h3>📈 Trends</h3>
            <div style="margin: 20px 0;">
"""
            if trends.get("review_slope") is not None:
                direction = "decreasing" if trends["review_slope"] < 0 else "increasing"
                emoji = "🟢" if trends["review_slope"] < 0 else "🔴"
                change = abs(trends["review_slope"]) * DAYS_PER_MONTH
                section_html += f"""
                <div style="margin: 10px 0;">
                    <strong>Review Time Trend:</strong>
                    <span class="trend-indicator {direction}">{emoji} {direction.upper()}</span>
                    <span style="margin-left: 10px;">Change: {change:.2f} hrs/month</span>
                </div>
"""
            if trends.get("merge_slope") is not None:
                direction = "decreasing" if trends["merge_slope"] < 0 else "increasing"
                emoji = "🟢" if trends["merge_slope"] < 0 else "🔴"
                change = abs(trends["merge_slope"]) * DAYS_PER_MONTH
                section_html += f"""
                <div style="margin: 10px 0;">
                    <strong>Merge Time Trend:</strong>
                    <span class="trend-indicator {direction}">{emoji} {direction.upper()}</span>
                    <span style="margin-left: 10px;">Change: {change:.2f} hrs/month</span>
                </div>
"""
            section_html += """
            </div>
"""

        # Charts
        if period_key in period_charts:
            charts = period_charts[period_key]
            if "trends" in charts:
                section_html += f"""
            <h3>📉 Trends Over Time</h3>
            <div class="chart-container">
                <img src="{charts['trends']}" alt="Trends Chart - {name}">
            </div>
"""
            if "distributions" in charts:
                section_html += f"""
            <h3>📊 Distribution Analysis</h3>
            <div class="chart-container">
                <img src="{charts['distributions']}" alt="Distribution Chart - {name}">
            </div>
"""

        section_html += """
        </div>
"""

    # Per-developer stats
    if per_dev:
        sorted_devs = sorted(per_dev.items(), key=lambda x: x[1]["pr_count"], reverse=True)

        section_html += """
        <h2>👥 Per Developer Statistics</h2>
"""
        if dev_chart_file:
            section_html += f"""
        <div class="chart-container">
            <img src="{dev_chart_file}" alt="Per Developer Statistics">
        </div>
"""

        section_html += """
        <table class="dev-table">
            <thead>
                <tr>
                    <th>Developer</th>
                    <th>PRs</th>
                    <th>Avg Review Time</th>
                    <th>Avg Merge Time</th>
                    <th>Avg Review → Merge</th>
                </tr>
            </thead>
            <tbody>
"""
        for author, stats in sorted_devs:
            review_time = f"{stats['avg_review_time']:.1f}h ({hours_to_days(stats['avg_review_time']):.1f}d)" if stats['avg_review_time'] else "N/A"
            merge_time = f"{stats['avg_merge_time']:.1f}h ({hours_to_days(stats['avg_merge_time']):.1f}d)" if stats['avg_merge_time'] else "N/A"
            review_to_merge = ""
            if stats['avg_review_time'] and stats['avg_merge_time']:
                diff = stats['avg_merge_time'] - stats['avg_review_time']
                review_to_merge = f"{diff:.1f}h ({hours_to_days(diff):.1f}d)"
            else:
                review_to_merge = "N/A"

            section_html += f"""
                <tr>
                    <td><strong>{author}</strong></td>
                    <td>{stats['pr_count']}</td>
                    <td>{review_time}</td>
                    <td>{merge_time}</td>
                    <td>{review_to_merge}</td>
                </tr>
"""
        section_html += """
            </tbody>
        </table>
"""

    section_html += """
            </div>
        </div>
"""

    return section_html


def generate_html_report(prs_by_repo: Dict[str, List[Dict]], output_dir: str) -> str:
    """Generate an HTML report with statistics and charts for all repositories."""
    from datetime import datetime, timezone

    report_time = datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M UTC")
    total_prs = sum(len(prs) for prs in prs_by_repo.values())

    # Start building HTML
    html = f"""<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>PR Review Time Analysis Report</title>
    <style>
        * {{ margin: 0; padding: 0; box-sizing: border-box; }}
        body {{
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            background: #f5f5f5;
            padding: 20px;
        }}
        .container {{
            max-width: 1400px;
            margin: 0 auto;
            background: white;
            padding: 40px;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
        }}
        h1 {{
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 2.5em;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
        }}
        .report-meta {{
            color: #7f8c8d;
            margin-bottom: 30px;
            font-size: 0.9em;
        }}
        h2 {{
            color: #2c3e50;
            margin-top: 40px;
            margin-bottom: 20px;
            font-size: 1.8em;
            border-left: 4px solid #3498db;
            padding-left: 15px;
        }}
        h3 {{
            color: #34495e;
            margin-top: 30px;
            margin-bottom: 15px;
            font-size: 1.4em;
        }}
        .stats-grid {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin: 20px 0;
        }}
        .stat-card {{
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        }}
        .stat-card.success {{
            background: linear-gradient(135deg, #11998e 0%, #38ef7d 100%);
        }}
        .stat-card.warning {{
            background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);
        }}
        .stat-card.info {{
            background: linear-gradient(135deg, #4facfe 0%, #00f2fe 100%);
        }}
        .stat-label {{
            font-size: 0.9em;
            opacity: 0.9;
            margin-bottom: 5px;
        }}
        .stat-value {{
            font-size: 2em;
            font-weight: bold;
        }}
        .stat-sub {{
            font-size: 0.85em;
            opacity: 0.8;
            margin-top: 5px;
        }}
        .chart-container {{
            margin: 30px 0;
            text-align: center;
        }}
        .chart-container img {{
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        }}
        .dev-table {{
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            background: white;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }}
        .dev-table th {{
            background: #3498db;
            color: white;
            padding: 12px;
            text-align: left;
            font-weight: 600;
        }}
        .dev-table td {{
            padding: 12px;
            border-bottom: 1px solid #ecf0f1;
        }}
        .dev-table tr:hover {{
            background: #f8f9fa;
        }}
        .period-section {{
            background: #f8f9fa;
            padding: 30px;
            border-radius: 8px;
            margin: 30px 0;
            border-left: 5px solid #3498db;
        }}
        .trend-indicator {{
            display: inline-block;
            padding: 5px 15px;
            border-radius: 20px;
            font-weight: bold;
            font-size: 0.9em;
        }}
        .trend-indicator.decreasing {{
            background: #d4edda;
            color: #155724;
        }}
        .trend-indicator.increasing {{
            background: #f8d7da;
            color: #721c24;
        }}
        .repo-section {{
            margin: 40px 0;
            background: white;
            border: 2px solid #e0e0e0;
            border-radius: 12px;
            overflow: hidden;
        }}
        .repo-header {{
            padding: 20px 30px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            cursor: pointer;
            display: flex;
            justify-content: space-between;
            align-items: center;
            transition: all 0.3s ease;
        }}
        .repo-header:hover {{
            background: linear-gradient(135deg, #5568d3 0%, #65398b 100%);
        }}
        .repo-header.combined {{
            background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);
        }}
        .repo-header.combined:hover {{
            background: linear-gradient(135deg, #df82ea 0%, #e4465b 100%);
        }}
        .repo-title {{
            font-size: 1.8em;
            font-weight: bold;
            margin: 0;
        }}
        .repo-meta {{
            font-size: 0.9em;
            opacity: 0.9;
            margin-top: 5px;
        }}
        .accordion-icon {{
            font-size: 1.5em;
            transition: transform 0.3s ease;
        }}
        .accordion-icon.open {{
            transform: rotate(180deg);
        }}
        .repo-content {{
            padding: 30px;
            display: none;
        }}
        .repo-content.open {{
            display: block;
        }}
        .global-dev-stats {{
            background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);
            padding: 30px;
            border-radius: 12px;
            margin: 30px 0;
            color: white;
        }}
        .global-dev-stats h2 {{
            color: white;
            border-left-color: white;
            margin-top: 0;
        }}
        .global-dev-stats .dev-table {{
            background: white;
            color: #333;
        }}
    </style>
</head>
<body>
    <div class="container">
        <h1 style="border-bottom: 3px solid #e74c3c;">📊 PR Review Time Analysis Report</h1>
        <div class="report-meta">
            Generated: {report_time} | Total Repositories: {len(prs_by_repo)} | Total PRs: {total_prs}
        </div>
"""

    # Calculate global per-developer stats across all repositories
    if len(prs_by_repo) > 1:
        all_prs = []
        for prs in prs_by_repo.values():
            all_prs.extend(prs)
        
        global_dev_stats = calculate_per_developer_stats(all_prs)
        if global_dev_stats:
            sorted_devs = sorted(global_dev_stats.items(), key=lambda x: x[1]["pr_count"], reverse=True)
            
            html += """
        <div class="global-dev-stats">
            <h2>👥 Global Developer Statistics (All Repositories)</h2>
            <p style="margin-bottom: 20px; opacity: 0.9;">Combined performance metrics across all repositories</p>
            
            <table class="dev-table">
                <thead>
                    <tr>
                        <th>Developer</th>
                        <th>Total PRs</th>
                        <th>Avg Review Time</th>
                        <th>Avg Merge Time</th>
                        <th>Avg Review → Merge</th>
                    </tr>
                </thead>
                <tbody>
"""
            for author, stats in sorted_devs:
                review_time = f"{stats['avg_review_time']:.1f}h ({hours_to_days(stats['avg_review_time']):.1f}d)" if stats['avg_review_time'] else "N/A"
                merge_time = f"{stats['avg_merge_time']:.1f}h ({hours_to_days(stats['avg_merge_time']):.1f}d)" if stats['avg_merge_time'] else "N/A"
                review_to_merge = ""
                if stats['avg_review_time'] and stats['avg_merge_time']:
                    diff = stats['avg_merge_time'] - stats['avg_review_time']
                    review_to_merge = f"{diff:.1f}h ({hours_to_days(diff):.1f}d)"
                else:
                    review_to_merge = "N/A"
                
                html += f"""
                    <tr>
                        <td><strong>{author}</strong></td>
                        <td>{stats['pr_count']}</td>
                        <td>{review_time}</td>
                        <td>{merge_time}</td>
                        <td>{review_to_merge}</td>
                    </tr>
"""
            html += """
                </tbody>
            </table>
        </div>
"""
        
        # Generate "All Repositories Combined" section
        html += generate_repo_section("All Repositories", all_prs, output_dir, is_combined=True, repo_index=0)
    
    # Generate individual repository sections
    for idx, repo_name in enumerate(sorted(prs_by_repo.keys()), start=1):
        html += generate_repo_section(repo_name, prs_by_repo[repo_name], output_dir, is_combined=False, repo_index=idx)
    
    html += """
    </div>
    
    <script>
        // Accordion functionality
        document.querySelectorAll('.repo-header').forEach(header => {{
            header.addEventListener('click', () => {{
                const content = header.nextElementSibling;
                const icon = header.querySelector('.accordion-icon');
                
                // Toggle current section
                content.classList.toggle('open');
                icon.classList.toggle('open');
            }});
        }});
        
        // Open first section by default
        const firstContent = document.querySelector('.repo-content');
        const firstIcon = document.querySelector('.accordion-icon');
        if (firstContent) {{
            firstContent.classList.add('open');
            firstIcon.classList.add('open');
        }}
    </script>
</body>
</html>
"""

    # Write HTML file
    report_path = os.path.join(output_dir, "report.html")
    with open(report_path, "w", encoding="utf-8") as f:
        f.write(html)

    return report_path


def main() -> None:
    args = parse_args()

    # Determine which CSV files to analyze
    csv_files = []
    if args.input:
        # Single file specified
        csv_files = [args.input]
        print(f"🔍 Loading PR data from {args.input}...")
    else:
        # Auto-detect CSV files in data directory
        print(f"🔍 Scanning {args.data_dir} for CSV files...")
        csv_files = find_csv_files(args.data_dir)

        if not csv_files:
            print(f"❌ No CSV files found in {args.data_dir}", file=sys.stderr)
            print(f"   Run 'python gh_pr_times.py --repos owner/repo' first to fetch data", file=sys.stderr)
            sys.exit(1)

        print(f"✓ Found {len(csv_files)} CSV file(s):")
        for csv_file in csv_files:
            print(f"  - {os.path.basename(csv_file)}")

    # Load data by repository
    prs_by_repo = load_prs_by_repository(csv_files)

    if not prs_by_repo:
        print("❌ No PR data found in CSV file(s).", file=sys.stderr)
        sys.exit(1)

    total_prs = sum(len(prs) for prs in prs_by_repo.values())
    print(f"✓ Loaded {total_prs} PRs total from {len(prs_by_repo)} repository/repositories")

    print("\n📊 Generating HTML report with charts...")
    if len(prs_by_repo) > 1:
        print(f"  └─ Creating combined view for all {len(prs_by_repo)} repositories")
    for repo_name in sorted(prs_by_repo.keys()):
        print(f"  └─ Processing {repo_name} ({len(prs_by_repo[repo_name])} PRs)")

    # Set matplotlib style
    plt.style.use(CHART_STYLE)

    # Generate HTML report (includes all stats and charts)
    report_path = generate_html_report(prs_by_repo, args.output_dir)

    print(f"\n✅ Analysis complete!")
    print(f"   📄 HTML Report: {report_path}")
    print(f"   📊 Charts: {args.output_dir}/")
    print(f"\n💡 Open {report_path} in your browser to view the full report!")


if __name__ == "__main__":
    main()
